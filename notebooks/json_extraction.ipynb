{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c4ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9936e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x, n = 5):\n",
    "    print(x.shape)\n",
    "    return x.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05cc5b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Straight to the Tree: Constituency Parsing with Neural Syntactic Distance.pdf.json',\n",
       " 'Exploring Optimism and Pessimism in Twitter Using Deep Learning.pdf.json',\n",
       " 'Streaming Principal Component Analysis in Noisy Settings.pdf.json',\n",
       " 'Oracle Complexity of Second-Order Methods for Finite-Sum Problems.pdf.json',\n",
       " 'Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization.pdf.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"talksumm/data/json\"\n",
    "files = os.listdir(folder)\n",
    "print(len(files))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a79c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fa783df34d41b986a2bff824dd4b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'metadata'])\n",
      "Straight to the Tree: Constituency Parsing with Neural Syntactic Distance.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'title', 'authors', 'emails', 'sections', 'references', 'referenceMentions', 'year', 'abstractText', 'creator'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for f in tqdm(files):\n",
    "    with open(folder + \"/\" + f) as doc:\n",
    "        data.append(json.load(doc))\n",
    "\n",
    "print(data[0].keys())\n",
    "print(data[0][\"name\"])\n",
    "data[0][\"metadata\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4745d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1651, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>emails</th>\n",
       "      <th>sections</th>\n",
       "      <th>references</th>\n",
       "      <th>referenceMentions</th>\n",
       "      <th>year</th>\n",
       "      <th>abstractText</th>\n",
       "      <th>creator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>META</td>\n",
       "      <td>Straight to the Tree: Constituency Parsing wit...</td>\n",
       "      <td>[Yikang Shen, Zhouhan Lin, Athul Paul Jacob, A...</td>\n",
       "      <td>[kang.shen@umontreal.ca,, zhouhan.lin@umontrea...</td>\n",
       "      <td>[{'heading': None, 'text': 'Proceedings of the...</td>\n",
       "      <td>[{'title': 'Globally normalized transition-bas...</td>\n",
       "      <td>[{'referenceID': 3, 'context': 'Parsing has be...</td>\n",
       "      <td>2018</td>\n",
       "      <td>In this work, we propose a novel constituency ...</td>\n",
       "      <td>LaTeX with hyperref package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>META</td>\n",
       "      <td>Exploring Optimism and Pessimism in Twitter Us...</td>\n",
       "      <td>[Cornelia Caragea, Liviu P. Dinu, Bogdan Dumitru]</td>\n",
       "      <td>[ccaragea@ksu.edu,, ldinu@fmi.unibuc.ro,, bogd...</td>\n",
       "      <td>[{'heading': None, 'text': 'Proceedings of the...</td>\n",
       "      <td>[{'title': 'TensorFlow: Large-scale machine le...</td>\n",
       "      <td>[{'referenceID': 22, 'context': 'Much has been...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Identifying optimistic and pessimistic viewpoi...</td>\n",
       "      <td>LaTeX with hyperref package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>META</td>\n",
       "      <td>Streaming Principal Component Analysis in Nois...</td>\n",
       "      <td>[Teodor V. Marinov, Poorya Mianjy, Raman Arora]</td>\n",
       "      <td>[&lt;tmarino2@cs.jhu.edu&gt;.]</td>\n",
       "      <td>[{'heading': '1. Introduction', 'text': 'Princ...</td>\n",
       "      <td>[{'title': 'First efficient convergence for st...</td>\n",
       "      <td>[{'referenceID': 2, 'context': 'from an unknow...</td>\n",
       "      <td>2018</td>\n",
       "      <td>We study streaming algorithms for principal co...</td>\n",
       "      <td>LaTeX with hyperref package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CRF</td>\n",
       "      <td>Oracle Complexity of Second-Order Methods for ...</td>\n",
       "      <td>[Yossi Arjevani, Ohad Shamir]</td>\n",
       "      <td>[Arjevani&lt;yossi.arjevani@weizmann.ac.il&gt;,, &lt;oh...</td>\n",
       "      <td>[{'heading': '1. Introduction', 'text': 'We co...</td>\n",
       "      <td>[{'title': 'A lower bound for the optimization...</td>\n",
       "      <td>[{'referenceID': 8, 'context': 'To study the c...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Finite-sum optimization problems are ubiquitou...</td>\n",
       "      <td>LaTeX with hyperref package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CRF</td>\n",
       "      <td>Doubly Greedy Primal-Dual Coordinate Descent f...</td>\n",
       "      <td>[Qi Lei, Ian E.H. Yen, Chao-yuan Wu, Inderjit ...</td>\n",
       "      <td>[&lt;leiqi@ices.utexas.edu&gt;,, &lt;eyan@cs.cmu.edu&gt;.]</td>\n",
       "      <td>[{'heading': '1. Introduction', 'text': 'Regul...</td>\n",
       "      <td>[{'title': 'Large-scale machine learning with ...</td>\n",
       "      <td>[{'referenceID': 2, 'context': 'Applications w...</td>\n",
       "      <td>2017</td>\n",
       "      <td>We consider the popular problem of sparse empi...</td>\n",
       "      <td>Preview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id source                                              title  \\\n",
       "0         0   META  Straight to the Tree: Constituency Parsing wit...   \n",
       "1         1   META  Exploring Optimism and Pessimism in Twitter Us...   \n",
       "2         2   META  Streaming Principal Component Analysis in Nois...   \n",
       "3         3    CRF  Oracle Complexity of Second-Order Methods for ...   \n",
       "4         4    CRF  Doubly Greedy Primal-Dual Coordinate Descent f...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Yikang Shen, Zhouhan Lin, Athul Paul Jacob, A...   \n",
       "1  [Cornelia Caragea, Liviu P. Dinu, Bogdan Dumitru]   \n",
       "2    [Teodor V. Marinov, Poorya Mianjy, Raman Arora]   \n",
       "3                      [Yossi Arjevani, Ohad Shamir]   \n",
       "4  [Qi Lei, Ian E.H. Yen, Chao-yuan Wu, Inderjit ...   \n",
       "\n",
       "                                              emails  \\\n",
       "0  [kang.shen@umontreal.ca,, zhouhan.lin@umontrea...   \n",
       "1  [ccaragea@ksu.edu,, ldinu@fmi.unibuc.ro,, bogd...   \n",
       "2                           [<tmarino2@cs.jhu.edu>.]   \n",
       "3  [Arjevani<yossi.arjevani@weizmann.ac.il>,, <oh...   \n",
       "4     [<leiqi@ices.utexas.edu>,, <eyan@cs.cmu.edu>.]   \n",
       "\n",
       "                                            sections  \\\n",
       "0  [{'heading': None, 'text': 'Proceedings of the...   \n",
       "1  [{'heading': None, 'text': 'Proceedings of the...   \n",
       "2  [{'heading': '1. Introduction', 'text': 'Princ...   \n",
       "3  [{'heading': '1. Introduction', 'text': 'We co...   \n",
       "4  [{'heading': '1. Introduction', 'text': 'Regul...   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{'title': 'Globally normalized transition-bas...   \n",
       "1  [{'title': 'TensorFlow: Large-scale machine le...   \n",
       "2  [{'title': 'First efficient convergence for st...   \n",
       "3  [{'title': 'A lower bound for the optimization...   \n",
       "4  [{'title': 'Large-scale machine learning with ...   \n",
       "\n",
       "                                   referenceMentions  year  \\\n",
       "0  [{'referenceID': 3, 'context': 'Parsing has be...  2018   \n",
       "1  [{'referenceID': 22, 'context': 'Much has been...  2018   \n",
       "2  [{'referenceID': 2, 'context': 'from an unknow...  2018   \n",
       "3  [{'referenceID': 8, 'context': 'To study the c...  2017   \n",
       "4  [{'referenceID': 2, 'context': 'Applications w...  2017   \n",
       "\n",
       "                                        abstractText  \\\n",
       "0  In this work, we propose a novel constituency ...   \n",
       "1  Identifying optimistic and pessimistic viewpoi...   \n",
       "2  We study streaming algorithms for principal co...   \n",
       "3  Finite-sum optimization problems are ubiquitou...   \n",
       "4  We consider the popular problem of sparse empi...   \n",
       "\n",
       "                       creator  \n",
       "0  LaTeX with hyperref package  \n",
       "1  LaTeX with hyperref package  \n",
       "2  LaTeX with hyperref package  \n",
       "3  LaTeX with hyperref package  \n",
       "4                      Preview  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.DataFrame.from_records([d[\"metadata\"] for d in data])\\\n",
    ".reset_index()\\\n",
    ".rename(columns = {\"index\":\"paper_id\"})\n",
    "show(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1994b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372550, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Straight to the Tree: Constituency Parsing wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>Proceedings of the 56th Annual Meeting of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Straight to the Tree: Constituency Parsing wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>Parsing has been useful for incorporating ling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Straight to the Tree: Constituency Parsing wit...</td>\n",
       "      <td>2</td>\n",
       "      <td>Neural network-based approaches relying on den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Straight to the Tree: Constituency Parsing wit...</td>\n",
       "      <td>3</td>\n",
       "      <td>Generally speaking, either these approaches pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Straight to the Tree: Constituency Parsing wit...</td>\n",
       "      <td>4</td>\n",
       "      <td>Corresponding authors: yikang.shen@umontreal.c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                              title  sentence  \\\n",
       "0         0  Straight to the Tree: Constituency Parsing wit...         0   \n",
       "1         0  Straight to the Tree: Constituency Parsing wit...         1   \n",
       "2         0  Straight to the Tree: Constituency Parsing wit...         2   \n",
       "3         0  Straight to the Tree: Constituency Parsing wit...         3   \n",
       "4         0  Straight to the Tree: Constituency Parsing wit...         4   \n",
       "\n",
       "                                                text  \n",
       "0  Proceedings of the 56th Annual Meeting of the ...  \n",
       "1  Parsing has been useful for incorporating ling...  \n",
       "2  Neural network-based approaches relying on den...  \n",
       "3  Generally speaking, either these approaches pr...  \n",
       "4  Corresponding authors: yikang.shen@umontreal.c...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_sentences = papers[[\"paper_id\", \"title\", \"sections\"]]\\\n",
    ".assign(full_text = lambda x: x.sections.map(lambda y: \"\\n\".join([z[\"text\"] for z in y])))\\\n",
    ".assign(sentences = lambda x: x.full_text.map(lambda y: list(enumerate(sent_tokenize(y)))))\\\n",
    ".explode(\"sentences\")\\\n",
    ".drop(columns = [\"sections\", \"full_text\"])\\\n",
    ".assign(\n",
    "    sentence = lambda x: x.sentences.map(lambda y: y[0]),\n",
    "    text = lambda x: x.sentences.map(lambda y: y[1].strip())\n",
    ")\\\n",
    ".drop(columns = \"sentences\")\\\n",
    ".reset_index(drop = True)\\\n",
    "# .rename(columns = {\"index\":\"paper\"})\n",
    "\n",
    "show(paper_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f03f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders.txt',\n",
       " 'Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers.txt',\n",
       " 'Analyzing the Robustness of Nearest Neighbors to Adversarial Examples.txt',\n",
       " 'How Much Information Does a Human Translator Add to the Original?.txt',\n",
       " 'An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank Approximation.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_folder = \"LongSumm/extractive_summaries/talksumm_summaries/\"\n",
    "files = os.listdir(summaries_folder)\n",
    "print(len(files))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a34ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ceb9794aa94f0ea8132084eca62a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders', ['6\\t23\\tFurther, we show that this model can learn a semantically meaningful hidden representation that can be used as a high-level control signal for manipulating tone, timbre, and dynamics during playback.\\n', '7\\t20\\tExplicitly, our two contributions to advance the state of generative audio modeling are: • A WaveNet-style autoencoder that learns temporal hidden codes to effectively capture longer term structure without external conditioning.\\n', '8\\t112\\t• NSynth: a large-scale dataset for exploring neural audio synthesis of musical notes.\\n', '9\\t43\\tThe primary motivation for our novel autoencoder structure follows from the recent advances in autoregressive models like WaveNet (van den Oord et al., 2016a) and SampleRNN (Mehri et al., 2016).\\n', '10\\t36\\tThey have proven to be effective at modeling short and medium scale (∼500ms) signals, but rely on external conditioning for longer-term dependencies.\\n', '15\\t16\\tWhile generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al., 2016).\\n', '18\\t23\\tInspired by the large, high-quality image datasets, NSynth is an order of magnitude larger than comparable public datasets (Humphrey, 2016).\\n', '19\\t32\\tIt consists of ∼300k foursecond annotated notes sampled at 16kHz from ∼1k harmonic musical instruments.\\n', '21\\t17\\tWe examine the tasks of reconstruction and interpolation, and analyze the learned space of embeddings.\\n', '22\\t15\\tFor qualitative evaluation, we include supplemental audio files for all examples mentioned in this paper.\\n', '36\\t22\\tNote that the decoder could completely ignore the deterministic encoding and degenerate to a standard unconditioned WaveNet.\\n', '41\\t32\\tThe temporal encoder model is a 30-layer nonlinear residual network of dilated convolutions followed by 1x1 convolutions.\\n', '60\\t14\\tThe number of channels grows from 128 to 1024 before a linear fully-connected layer creates a single 19841 dimensional hidden vector (Z) to match that of the WaveNet autoencoder.\\n', '64\\t14\\tWe found that training on the log magnitude of the power spectra, peak normalized to be between 0 and 1, correlated better with perceptual distortion.\\n', '68\\t15\\tAs a final heuristic, we weighted the MSE loss, starting at 10 for 0Hz and decreasing linearly to 1 at 4000Hz and above.\\n', '71\\t23\\tThe baseline models commonly use a learning rate of 1e-4, while the WaveNet models use a schedule, starting at 2e-4 and descending to 6e-5, 2e-5, and 6e-6 at iterations 120k, 180k, and 240k respectively.\\n', '72\\t27\\tThe baseline models train asynchronously for 1800k iterations with a batch size of 8.\\n', '97\\t15\\tWe have included supplemental audio examples of every plot and encourage the reader to listen along as they read.\\n', '131\\t19\\tInspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Supplemental).\\n', '134\\t31\\tThe classifier is ∼70% more successful at extracting pitch from the reconstructed WaveNet samples than the baseline and several points higher for predicting quality information, giving an accuracy roughly equal to the original audio.\\n', '141\\t33\\tFor example, in the interpolated note between the bass and flute (Figure 3, column 2), we can hear and see that both the baseline and WaveNet models blend the harmonic structure of the two instruments while imposing the amplitude envelope of the bass note onto the upper harmonics of the flute note.\\n', '143\\t24\\tThis sound captures expressive aspects of the timbre and dynamics of both the bass and flute, but is distinctly separate from either original note.\\n', '146\\t29\\tWe see this phenomenon again in the interpolation between flute and organ (Figure 3, column 4).\\n', '148\\t53\\tThe WaveNet model adds additional harmonics as well as a sub-harmonic to the original flute note, all while preserving phase relationships (B).\\n', '176\\t16\\tFigure 5 shows correlations for several instruments across their entire 88 note range at velocity 127.\\n', '177\\t15\\tWe see that each instrument has a unique partitioning into two or more registers over which notes of different pitches have similar embeddings.\\n', '180\\t72\\tIn this paper, we have introduced a WaveNet autoencoder model that captures long term structure without the need for external conditioning and demonstrated its effectiveness on the new NSynth dataset for generative modeling of audio.\\n', '181\\t19\\tThe WaveNet autoencoder that we describe is a powerful representation for which there remain multiple avenues of exploration.\\n', '186\\t27\\tWe encourage the broader community to use NSynth as a benchmark and entry point into audio machine learning.\\n', '187\\t41\\tWe also view NSynth as a building block for future datasets and envision a high-quality multi-note dataset for tasks like generation and transcription that involve learning complex language-like dependencies.\\n'])\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for f in tqdm(files):\n",
    "    with open(summaries_folder + \"/\" + f) as doc:\n",
    "        data.append((f.replace(\".txt\", \"\").strip(), doc.readlines()))\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90654a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529dac307b6e45ff9f97cd45d9925b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '6',\n",
       "  '23',\n",
       "  'Further, we show that this model can learn a semantically meaningful hidden representation that can be used as a high-level control signal for manipulating tone, timbre, and dynamics during playback.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '7',\n",
       "  '20',\n",
       "  'Explicitly, our two contributions to advance the state of generative audio modeling are: • A WaveNet-style autoencoder that learns temporal hidden codes to effectively capture longer term structure without external conditioning.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '8',\n",
       "  '112',\n",
       "  '• NSynth: a large-scale dataset for exploring neural audio synthesis of musical notes.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '9',\n",
       "  '43',\n",
       "  'The primary motivation for our novel autoencoder structure follows from the recent advances in autoregressive models like WaveNet (van den Oord et al., 2016a) and SampleRNN (Mehri et al., 2016).'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '10',\n",
       "  '36',\n",
       "  'They have proven to be effective at modeling short and medium scale (∼500ms) signals, but rely on external conditioning for longer-term dependencies.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '15',\n",
       "  '16',\n",
       "  'While generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al., 2016).'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '18',\n",
       "  '23',\n",
       "  'Inspired by the large, high-quality image datasets, NSynth is an order of magnitude larger than comparable public datasets (Humphrey, 2016).'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '19',\n",
       "  '32',\n",
       "  'It consists of ∼300k foursecond annotated notes sampled at 16kHz from ∼1k harmonic musical instruments.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '21',\n",
       "  '17',\n",
       "  'We examine the tasks of reconstruction and interpolation, and analyze the learned space of embeddings.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '22',\n",
       "  '15',\n",
       "  'For qualitative evaluation, we include supplemental audio files for all examples mentioned in this paper.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '36',\n",
       "  '22',\n",
       "  'Note that the decoder could completely ignore the deterministic encoding and degenerate to a standard unconditioned WaveNet.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '41',\n",
       "  '32',\n",
       "  'The temporal encoder model is a 30-layer nonlinear residual network of dilated convolutions followed by 1x1 convolutions.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '60',\n",
       "  '14',\n",
       "  'The number of channels grows from 128 to 1024 before a linear fully-connected layer creates a single 19841 dimensional hidden vector (Z) to match that of the WaveNet autoencoder.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '64',\n",
       "  '14',\n",
       "  'We found that training on the log magnitude of the power spectra, peak normalized to be between 0 and 1, correlated better with perceptual distortion.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '68',\n",
       "  '15',\n",
       "  'As a final heuristic, we weighted the MSE loss, starting at 10 for 0Hz and decreasing linearly to 1 at 4000Hz and above.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '71',\n",
       "  '23',\n",
       "  'The baseline models commonly use a learning rate of 1e-4, while the WaveNet models use a schedule, starting at 2e-4 and descending to 6e-5, 2e-5, and 6e-6 at iterations 120k, 180k, and 240k respectively.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '72',\n",
       "  '27',\n",
       "  'The baseline models train asynchronously for 1800k iterations with a batch size of 8.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '97',\n",
       "  '15',\n",
       "  'We have included supplemental audio examples of every plot and encourage the reader to listen along as they read.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '131',\n",
       "  '19',\n",
       "  'Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Supplemental).'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '134',\n",
       "  '31',\n",
       "  'The classifier is ∼70% more successful at extracting pitch from the reconstructed WaveNet samples than the baseline and several points higher for predicting quality information, giving an accuracy roughly equal to the original audio.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '141',\n",
       "  '33',\n",
       "  'For example, in the interpolated note between the bass and flute (Figure 3, column 2), we can hear and see that both the baseline and WaveNet models blend the harmonic structure of the two instruments while imposing the amplitude envelope of the bass note onto the upper harmonics of the flute note.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '143',\n",
       "  '24',\n",
       "  'This sound captures expressive aspects of the timbre and dynamics of both the bass and flute, but is distinctly separate from either original note.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '146',\n",
       "  '29',\n",
       "  'We see this phenomenon again in the interpolation between flute and organ (Figure 3, column 4).'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '148',\n",
       "  '53',\n",
       "  'The WaveNet model adds additional harmonics as well as a sub-harmonic to the original flute note, all while preserving phase relationships (B).'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '176',\n",
       "  '16',\n",
       "  'Figure 5 shows correlations for several instruments across their entire 88 note range at velocity 127.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '177',\n",
       "  '15',\n",
       "  'We see that each instrument has a unique partitioning into two or more registers over which notes of different pitches have similar embeddings.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '180',\n",
       "  '72',\n",
       "  'In this paper, we have introduced a WaveNet autoencoder model that captures long term structure without the need for external conditioning and demonstrated its effectiveness on the new NSynth dataset for generative modeling of audio.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '181',\n",
       "  '19',\n",
       "  'The WaveNet autoencoder that we describe is a powerful representation for which there remain multiple avenues of exploration.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '186',\n",
       "  '27',\n",
       "  'We encourage the broader community to use NSynth as a benchmark and entry point into audio machine learning.'],\n",
       " ['Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders',\n",
       "  '187',\n",
       "  '41',\n",
       "  'We also view NSynth as a building block for future datasets and envision a high-quality multi-note dataset for tasks like generation and transcription that involve learning complex language-like dependencies.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[[t] + l.strip().split(\"\\t\") for l in d] for t, d in tqdm(data)]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0173ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50242, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Audio Synthesis of Musical Notes  with ...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Further, we show that this model can learn a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Audio Synthesis of Musical Notes  with ...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>Explicitly, our two contributions to advance t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neural Audio Synthesis of Musical Notes  with ...</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>• NSynth: a large-scale dataset for exploring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural Audio Synthesis of Musical Notes  with ...</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>The primary motivation for our novel autoencod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Audio Synthesis of Musical Notes  with ...</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>They have proven to be effective at modeling s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title sentence_index  \\\n",
       "0  Neural Audio Synthesis of Musical Notes  with ...              6   \n",
       "1  Neural Audio Synthesis of Musical Notes  with ...              7   \n",
       "2  Neural Audio Synthesis of Musical Notes  with ...              8   \n",
       "3  Neural Audio Synthesis of Musical Notes  with ...              9   \n",
       "4  Neural Audio Synthesis of Musical Notes  with ...             10   \n",
       "\n",
       "  sentence_score                                               text  \n",
       "0             23  Further, we show that this model can learn a s...  \n",
       "1             20  Explicitly, our two contributions to advance t...  \n",
       "2            112  • NSynth: a large-scale dataset for exploring ...  \n",
       "3             43  The primary motivation for our novel autoencod...  \n",
       "4             36  They have proven to be effective at modeling s...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = pd.DataFrame(sum(data, start = []), columns = [\"title\", \"sentence_index\", \"sentence_score\", \"text\"])\n",
    "show(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d2e101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18,718,360,488'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{372564 * 50242:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e7e691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries.groupby(\"title\").count().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "618f055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372550, 5)\n",
      "CPU times: user 3.7 s, sys: 68 ms, total: 3.76 s\n",
      "Wall time: 3.76 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence_score</th>\n",
       "      <th>in_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Proceedings of the 56th Annual Meeting of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Parsing has been useful for incorporating ling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Neural network-based approaches relying on den...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Generally speaking, either these approaches pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Corresponding authors: yikang.shen@umontreal.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>†Work done while at Microsoft Research, Montreal.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>the sequence of transitions in a transition-ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Transition-based models decompose the structur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>This enables fast greedy decoding but also lea...</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Solutions to this problem usually complexify t...</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>On the other hand, chartbased models can incor...</td>\n",
       "      <td>34</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>In this paper, we propose a novel, fully-paral...</td>\n",
       "      <td>98</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>To construct a parse tree from a sentence, one...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>The syntactic distances are defined for each p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>The order induced by the syntactic distances f...</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>Therefore, our model is trained to reproduce t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Crucially, our model works in parallel: the es...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Along with the distances, we also train the mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>Our model is fully parallel and thus does not ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>Mapping from syntactic distances to a tree can...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id  sentence                                               text  \\\n",
       "0          0         0  Proceedings of the 56th Annual Meeting of the ...   \n",
       "1          0         1  Parsing has been useful for incorporating ling...   \n",
       "2          0         2  Neural network-based approaches relying on den...   \n",
       "3          0         3  Generally speaking, either these approaches pr...   \n",
       "4          0         4  Corresponding authors: yikang.shen@umontreal.c...   \n",
       "5          0         5  †Work done while at Microsoft Research, Montreal.   \n",
       "6          0         6  the sequence of transitions in a transition-ba...   \n",
       "7          0         7  Transition-based models decompose the structur...   \n",
       "8          0         8  This enables fast greedy decoding but also lea...   \n",
       "9          0         9  Solutions to this problem usually complexify t...   \n",
       "10         0        10  On the other hand, chartbased models can incor...   \n",
       "11         0        11  In this paper, we propose a novel, fully-paral...   \n",
       "12         0        12  To construct a parse tree from a sentence, one...   \n",
       "13         0        13  The syntactic distances are defined for each p...   \n",
       "14         0        14  The order induced by the syntactic distances f...   \n",
       "15         0        15  Therefore, our model is trained to reproduce t...   \n",
       "16         0        16  Crucially, our model works in parallel: the es...   \n",
       "17         0        17  Along with the distances, we also train the mo...   \n",
       "18         0        18  Our model is fully parallel and thus does not ...   \n",
       "19         0        19  Mapping from syntactic distances to a tree can...   \n",
       "\n",
       "   sentence_score  in_summary  \n",
       "0             NaN       False  \n",
       "1             NaN       False  \n",
       "2             NaN       False  \n",
       "3             NaN       False  \n",
       "4             NaN       False  \n",
       "5             NaN       False  \n",
       "6             NaN       False  \n",
       "7             NaN       False  \n",
       "8              17        True  \n",
       "9              16        True  \n",
       "10             34        True  \n",
       "11             98        True  \n",
       "12            NaN       False  \n",
       "13            NaN       False  \n",
       "14             26        True  \n",
       "15            NaN       False  \n",
       "16            NaN       False  \n",
       "17            NaN       False  \n",
       "18            NaN       False  \n",
       "19            NaN       False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "labels = paper_sentences\\\n",
    ".assign(clean_text = lambda x: x.text.str.replace(\"\\W\", \"\", regex = True).str.lower())\\\n",
    ".merge(\n",
    "    summaries\\\n",
    "    .assign(clean_text = lambda x: x.text.str.replace(\"\\W\", \"\", regex = True).str.lower())\\\n",
    "    .drop(columns = \"text\"), \n",
    "    how = \"left\", \n",
    "    left_on = [\"title\", \"clean_text\"], \n",
    "    right_on = [\"title\", \"clean_text\"],\n",
    "#     left_on = [\"title\", \"text\"], \n",
    "#     right_on = [\"title\", \"text\"],\n",
    ")\\\n",
    ".groupby([\"paper_id\", \"sentence\"]).head(1)\\\n",
    ".assign(in_summary = lambda x: ~x.sentence_score.isna())\\\n",
    ".drop(columns = [\"sentence_index\", \"title\", \"clean_text\"])\n",
    "\n",
    "show(labels, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846aca7",
   "metadata": {},
   "source": [
    "# exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09390fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248374</th>\n",
       "      <td>1101</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248372</th>\n",
       "      <td>1101</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248371</th>\n",
       "      <td>1101</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248370</th>\n",
       "      <td>1101</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248369</th>\n",
       "      <td>1101</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248368</th>\n",
       "      <td>1101</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248367</th>\n",
       "      <td>1101</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248366</th>\n",
       "      <td>1101</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248365</th>\n",
       "      <td>1101</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248364</th>\n",
       "      <td>1101</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248363</th>\n",
       "      <td>1101</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248362</th>\n",
       "      <td>1101</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248361</th>\n",
       "      <td>1101</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248360</th>\n",
       "      <td>1101</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248359</th>\n",
       "      <td>1101</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248358</th>\n",
       "      <td>1101</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248357</th>\n",
       "      <td>1101</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248356</th>\n",
       "      <td>1101</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248355</th>\n",
       "      <td>1101</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id  sentence  text\n",
       "0              0         0     1\n",
       "248374      1101       114     1\n",
       "248372      1101       112     1\n",
       "248371      1101       111     1\n",
       "248370      1101       110     1\n",
       "248369      1101       109     1\n",
       "248368      1101       108     1\n",
       "248367      1101       107     1\n",
       "248366      1101       106     1\n",
       "248365      1101       105     1\n",
       "248364      1101       104     1\n",
       "248363      1101       103     1\n",
       "248362      1101       102     1\n",
       "248361      1101       101     1\n",
       "248360      1101       100     1\n",
       "248359      1101        99     1\n",
       "248358      1101        98     1\n",
       "248357      1101        97     1\n",
       "248356      1101        96     1\n",
       "248355      1101        95     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.groupby([\"paper_id\", \"sentence\"]).text.count().reset_index().sort_values(\"text\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d68eb94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0,\n",
       "        'Deep linear networks (DLN) are neural networks that have multiple hidden layers but have no nonlinearities between layers.'],\n",
       "       [1,\n",
       "        'That is, for given data points {x(i)}Ni=1 the outputs of such networks are computed via a series\\nŷ(i) = WLWL−1 · · ·W1x(i)\\nof matrix multiplications.'],\n",
       "       [2,\n",
       "        'Given a target y(i) for the ith data point and a pairwise loss function `(ŷ(i),y(i)), forming the usual summation\\nL(W1, .'],\n",
       "       [3, '.'],\n",
       "       [4, '.'],\n",
       "       [5,\n",
       "        ',WL) = 1\\nN N∑ i=1 `(ŷ(i),y(i)) (1)\\nthen yields the total loss.'],\n",
       "       [6,\n",
       "        'Such networks have few direct applications, but they frequently appear as a class of toy models used to understand the loss surfaces of deep neural networks (Saxe et al., 2014; Kawaguchi, 2016; Lu & Kawaguchi, 2017; Hardt & Ma, 2017).'],\n",
       "       [7,\n",
       "        'For example, numerical experiments indicate that DLNs exhibit some behavior that resembles the behavior of\\n*Equal contribution 1Department of Mathematics, Loyola Marymount University, Los Angeles, CA 90045, USA 2Department of Mathematics and Statistics, California State University, Long Beach, Long Beach, CA 90840, USA.'],\n",
       "       [8,\n",
       "        'Correspondence to: Thomas Laurent <tlaurent@lmu.edu>, James H. von Brecht <james.vonbrecht@csulb.edu>.'],\n",
       "       [9,\n",
       "        'Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.'],\n",
       "       [10, 'Copyright 2018 by the author(s).'],\n",
       "       [11,\n",
       "        'deep nonlinear networks during training (Saxe et al., 2014).'],\n",
       "       [12,\n",
       "        'Results of this sort provide a small piece of evidence that DLNs can provide a decent simplified model of more realistic networks with nonlinearities.'],\n",
       "       [13,\n",
       "        'From an analytical point-of-view, the simplicity of DLNs allows for a rigorous, in-depth study of their loss surfaces.'],\n",
       "       [14,\n",
       "        'These models typically employ a convex loss function `(ŷ,y), and so with one layer (i.e.'],\n",
       "       [15,\n",
       "        'L = 1) the loss L(W1) is convex and the resulting optimization problem (1) has no sub-optimal local minimizers.'],\n",
       "       [16, 'With multiple layers (i.e.'],\n",
       "       [17, 'L ≥ 2) the loss L(W1, .'],\n",
       "       [18, '.'],\n",
       "       [19, '.'],\n",
       "       [20,\n",
       "        ',WL) is not longer convex, and so the question of paramount interest concerns whether this addition of depth and the subsequent loss of convexity creates sub-optimal local minimizers.'],\n",
       "       [21,\n",
       "        'Indeed, most analytical treatments of DLNs focus on this question.'],\n",
       "       [22,\n",
       "        'We resolve this question in full for arbitrary convex differentiable loss functions.'],\n",
       "       [23,\n",
       "        'Specifically, we consider deep linear networks satisfying the two following hypotheses:\\n(i) The loss function ŷ 7→ `(y, ŷ) is convex and differentiable.'],\n",
       "       [24,\n",
       "        '(ii) The thinnest layer is either the input layer or the output layer.'],\n",
       "       [25,\n",
       "        'Many networks of interest satisfy both of these hypotheses.'],\n",
       "       [26,\n",
       "        'The first hypothesis (i) holds for nearly all network criteria, such as the mean squared error loss, the logistic loss or the cross entropy loss, that appear in applications.'],\n",
       "       [27,\n",
       "        'In a classification scenario, the second hypothesis (ii) holds whenever each hidden layer has more neurons than the number of classes.'],\n",
       "       [28,\n",
       "        'Thus both hypotheses are often satisfied when using a deep linear network (1) to model its nonlinear counterpart.'],\n",
       "       [29,\n",
       "        'In any such situation we resolve the deep linear problem in its entirety.'],\n",
       "       [30, 'Theorem 1.'],\n",
       "       [31,\n",
       "        'If hypotheses (i) and (ii) hold then (1) has no sub-optimal minimizers, i.e.'],\n",
       "       [32, 'any local minimum is global.'],\n",
       "       [33, 'We provide a short, transparent proof of this result.'],\n",
       "       [34,\n",
       "        'It is easily accessible to any reader with a basic understanding of the singular value decomposition, and in particular, it does not rely on any sophisticated machinery from either optimization or linear algebra.'],\n",
       "       [35,\n",
       "        'Moreover, this theorem is the strongest possible in the following sense — Theorem 2.'],\n",
       "       [36,\n",
       "        'There exists a convex, Lipschitz but not differentiable function ŷ 7→ `(y, ŷ) for which (1) has sub-optimal\\nlocal minimizers.'],\n",
       "       [37,\n",
       "        'In other words, we have a (perhaps surprising) hard limit on how far “local equals global” results can reach; differentiability of the loss is essential.'],\n",
       "       [38,\n",
       "        'Many prior analytical treatments of DLNs focus on similar questions.'],\n",
       "       [39,\n",
       "        'For instance, both (Baldi & Hornik, 1989) and (Baldi & Lu, 2012) consider “deep” linear networks with two layers (i.e.'],\n",
       "       [40, 'L = 2) and a mean squared error loss criterion.'],\n",
       "       [41,\n",
       "        'They provide a “local equals global” result under some relatively mild assumptions on the data and targets.'],\n",
       "       [42,\n",
       "        'More recently, (Kawaguchi, 2016) proved that deep linear networks with arbitrary number of layers and with mean squared error loss do not have sub-optimal local minima under certain structural assumptions on the data and targets.'],\n",
       "       [43,\n",
       "        'The follow-up (Lu & Kawaguchi, 2017) futher simplifies the proof of this result and weakens the structural assumptions.'],\n",
       "       [44,\n",
       "        'Specifically, this result shows that the loss (1) associated with a deep linear network has no sub-optimal local minima provided all of assumptions\\n(i) The loss `(ŷ(i),y(i)) = ‖y(i) − ŷ(i)‖2 is the mean squared error loss criterion;\\n(ii) The data matrixX = [x(1), .'],\n",
       "       [45, '.'],\n",
       "       [46, '.'],\n",
       "       [47,\n",
       "        ',x(N)] has full row rank;\\n(iii) The target matrix Y = [y(1), .'],\n",
       "       [48, '.'],\n",
       "       [49, '.'],\n",
       "       [50, ',y(N)] has full row rank;\\nare satisfied.'],\n",
       "       [51,\n",
       "        'Compared to our result, (Lu & Kawaguchi, 2017) therefore allows for the hidden layers of the network to be thinner than the input and output layers.'],\n",
       "       [52,\n",
       "        'However, our result applies to network equipped with any differentiable convex loss (in fact any differentiable loss L for which firstorder optimality implies global optimality) and we do not require any assumption on the data and targets.'],\n",
       "       [53,\n",
       "        'Our proof is also shorter and much more elementary by comparison.'],\n",
       "       [54,\n",
       "        'Theorem 1 follows as a simple consequence of a more general theorem concerning real-valued functions that take as input a product of matrices.'],\n",
       "       [55,\n",
       "        'That is, we view the deep linear problem as a specific instance of the following more general problem.'],\n",
       "       [56,\n",
       "        'Let Mm×n denote the space of m × n real matrices, and let f : MdL×d0 → R denote any differentiable function that takes dL × d0 matrices as input.'],\n",
       "       [57,\n",
       "        'For any such function we may then consider both the single-layer optimization\\n(P1) { Minimize f(A) over all A in MdL×d0\\nas well as the analogous problem\\n(P2)  Minimize f(WLWL−1 · · ·W1) over all L-tuples (W1, .'],\n",
       "       [58, '.'],\n",
       "       [59, '.'],\n",
       "       [60,\n",
       "        ',WL) in Md1×d0 × · · · ×MdL×dL−1\\nthat corresponds to a multi-layer deep linear optimization.'],\n",
       "       [61,\n",
       "        'In other words, in (P2) we consider the task of optimizing f over those matrices A ∈MdL×d0 that can be realized by an L-fold product\\nA = WLWL−1 · · ·W1 W` ∈Md`×d`−1 (2)\\nof matrices.'],\n",
       "       [62,\n",
       "        'We may then ask how the parametrization (2) of A as a product of matrices affects the minimization of f, or in other words, whether the problems (P1) and (P2) have similar structure.'],\n",
       "       [63,\n",
       "        'At heart, the use of DLNs to model nonlinear neural networks centers around this question.'],\n",
       "       [64,\n",
       "        'Any notion of structural similarity between (P1) and (P2) should require that their global minima coincide.'],\n",
       "       [65, 'As a matrix of the form (2) has rank at most min{d0, .'],\n",
       "       [66, '.'],\n",
       "       [67, '.'],\n",
       "       [68,\n",
       "        ', dL}, we must impose the structural requirement\\nmin{d1, .'],\n",
       "       [69, '.'],\n",
       "       [70, '.'],\n",
       "       [71,\n",
       "        ', dL−1} ≥ min{dL, d0} (3)\\nin order to guarantee that (2) does, in fact, generate the full space of dL × d0 matrices.'],\n",
       "       [72,\n",
       "        'Under this assumption we shall prove the following quite general theorem.'],\n",
       "       [73, 'Theorem 3.'],\n",
       "       [74,\n",
       "        'Assume that f(A) is any differentiable function and that the structural condition (3) holds.'],\n",
       "       [75, 'Then at any local minimizer ( Ŵ1, .'],\n",
       "       [76, '.'],\n",
       "       [77, '.'],\n",
       "       [78,\n",
       "        ', ŴL ) of (P2) the optimality condition\\n∇f ( Â ) = 0 Â := ŴLŴL−1 · · · Ŵ1\\nis satisfied.'],\n",
       "       [79, 'Theorem 1 follows as a simple consequence of this theorem.'],\n",
       "       [80,\n",
       "        'The first hypothesis (i) of theorem 1 shows that the total loss (1) takes the form\\nL(W1, .'],\n",
       "       [81, '.'],\n",
       "       [82, '.'],\n",
       "       [83,\n",
       "        ',WL) = f(WL · · ·W1)\\nfor f(A) some convex and differentiable function.'],\n",
       "       [84,\n",
       "        'The structural hypothesis (3) is equivalent to the second hypothesis (ii) of theorem 1, and so we can directly apply theorem 3 to conclude that a local minimum ( Ŵ1, .'],\n",
       "       [85, '.'],\n",
       "       [86, '.'],\n",
       "       [87,\n",
       "        ', ŴL ) of L corresponds to a critical point Â = ŴL · · · Ŵ1 of f(A), and since f(A) is convex, this critical point is necessarily a global minimum.'],\n",
       "       [88,\n",
       "        'Before turning to the proof of theorem 3 we recall a bit of notation and provide a calculus lemma.'],\n",
       "       [89,\n",
       "        'Let\\n〈A,B〉fro := Tr(ATB) = ∑ i ∑ j AijBij and\\n‖A‖2fro := 〈A,A〉fro\\ndenote the Frobenius dot product and the Frobenius norm, respectively.'],\n",
       "       [90,\n",
       "        'Also, recall that for a differentiable function φ : Mm×n 7→ R its gradient∇φ(A) ∈Mm×n is the unique matrix so that the equality\\nφ(A+H) = φ(A) + 〈∇φ(A), H〉fro + o (‖H‖fro) (4)\\nholds.'],\n",
       "       [91, 'If F (W1, .'],\n",
       "       [92, '.'],\n",
       "       [93, '.'],\n",
       "       [94,\n",
       "        ',WL) := f(WL · · ·W1) denotes the objective of interest in (P2) the following lemma gives the partial derivatives of F as a function of its arguments.'],\n",
       "       [95, 'Lemma 1.'],\n",
       "       [96, 'The partial derivatives of F are given by\\n∇W1F (W1, .'],\n",
       "       [97, '.'],\n",
       "       [98, '.'],\n",
       "       [99, ',WL) = WT2,+∇f ( A ) ,\\n∇WkF (W1, .'],\n",
       "       [100, '.'],\n",
       "       [101, '.'],\n",
       "       [102, ',WL) = WTk+1,+∇f(A)WTk−1,−, ∇WLF (W1, .'],\n",
       "       [103, '.'],\n",
       "       [104, '.'],\n",
       "       [105,\n",
       "        ',WL) = ∇f ( A ) WTL−1,−,\\nwhere A stands for the full product A := WL · · ·W1 and Wk,+,Wk,− are the truncated products\\nWk,+ := WL · · ·Wk, Wk,− := Wk · · ·W1.'],\n",
       "       [106, '(5)\\nProof.'],\n",
       "       [107, 'The definition (4) implies\\nF (W1, .'],\n",
       "       [108, '.'],\n",
       "       [109, '.'],\n",
       "       [110, ',Wk−1,Wk +H,Wk+1, .'],\n",
       "       [111, '.'],\n",
       "       [112, '.'],\n",
       "       [113,\n",
       "        ',WL) = f ( A+Wk+1,+HWk−1,− ) = f(A) + 〈∇f(A),Wk+1,+HWk−1,−〉fro + o ( ‖H‖fro ) .'],\n",
       "       [114,\n",
       "        'Using the cyclic property Tr(ABC) = Tr(CAB) of the trace then gives\\n〈∇f(A) , Wk+1,+HWk−1,− 〉fro = Tr ( ∇f(A)TWk+1,+HWk−1,− ) = Tr ( Wk−1,−∇f(A)TWk+1,+H\\n) = 〈WTk+1,+∇f(A)WTk−1,− , H 〉fro\\nwhich, in light of (4), gives the desired formula for∇WkF .'],\n",
       "       [115, 'The formulas for∇W1F and∇WLF are obtained similarly.'],\n",
       "       [116,\n",
       "        'Proof of Theorem 3: To prove theorem 3 it suffices to assume that dL ≥ d0 without loss of generality.'],\n",
       "       [117,\n",
       "        'This follows from the simple observation that\\ng ( A ) := f ( AT )\\ndefines a differentiable function of d0 × dL matrices for f(A) any differentiable function of dL × d0 matrices.'],\n",
       "       [118, 'As a point ( W1, .'],\n",
       "       [119, '.'],\n",
       "       [120, '.'],\n",
       "       [121,\n",
       "        ',WL ) defines a local minimum of\\nf ( WLWL−1 · · ·W1 ) if and only if ( WT1 , .'],\n",
       "       [122, '.'],\n",
       "       [123, '.'],\n",
       "       [124,\n",
       "        ',W T L ) defines\\na minimum of g ( V1 · · ·VL−1VL ) , the theorem for the case dL < d0 follows by appealing to its dL ≥ d0 instance.'],\n",
       "       [125,\n",
       "        'It\\ntherefore suffices to assume that dL ≥ d0, and by the structural assumption that dk ≥ d0, throughout the remainder of the proof.'],\n",
       "       [126, 'Consider any local minimizer ( Ŵ1, .'],\n",
       "       [127, '.'],\n",
       "       [128, '.'],\n",
       "       [129,\n",
       "        ', ŴL ) of F and denote by Â, Ŵk,+ and Ŵk,− the corresponding full and truncated products (c.f.'],\n",
       "       [130, '(5)).'],\n",
       "       [131,\n",
       "        'By definition of a local minimizer there exists some ε0 > 0 so that\\nF (W1, .'],\n",
       "       [132, '.'],\n",
       "       [133, '.'],\n",
       "       [134, ',WL) ≥ F (Ŵ1, .'],\n",
       "       [135, '.'],\n",
       "       [136, '.'],\n",
       "       [137,\n",
       "        ', ŴL) = f ( Â ) (6)\\nwhenever the family of inequalities\\n‖W` − Ŵ`‖fro ≤ ε0 for all 1 ≤ ` ≤ L\\nall hold.'],\n",
       "       [138,\n",
       "        'Moreover, lemma 1 yields (i) 0 = ŴT2,+∇f ( Â ) ,\\n(ii) 0 = ŴTk+1,+∇f ( Â ) ŴTk−1,− ∀ 2 ≤ k ≤ L− 1,\\n(iii) 0 = ∇f ( Â ) ŴTL−1,−.'],\n",
       "       [139,\n",
       "        '(7)\\nsince all partial derivatives must vanish at a local minimum.'],\n",
       "       [140, 'If ŴL−1,− has a trivial kernel, i.e.'],\n",
       "       [141, 'ker(ŴL−1,−) = {0}, then the theorem follows easily.'],\n",
       "       [142,\n",
       "        'The critical point condition (7) part (iii) implies\\nŴL−1,−∇f ( Â )T = 0,\\nand since ŴL−1,− has a trivial kernel this implies ∇f ( Â ) = ∇f ( ŴLŴL−1 · · · Ŵ1 ) = 0 as desired.'],\n",
       "       [143,\n",
       "        'The remainder of the proof concerns the case that ŴL−1,− has a nontrivial kernel.'],\n",
       "       [144,\n",
       "        'The main idea is to use this nontrivial kernel to construct a family of infinitesimal perturbations of the local minimizer ( Ŵ1, .'],\n",
       "       [145, '.'],\n",
       "       [146, '.'],\n",
       "       [147,\n",
       "        ', ŴL ) that leaves the overall product ŴL · · · Ŵ1 unchanged.'],\n",
       "       [148, 'In other words, the family of perturbations ( W̃1, .'],\n",
       "       [149, '.'],\n",
       "       [150, '.'],\n",
       "       [151, ', W̃L ) satisfy\\n‖W̃` − Ŵ`‖fro ≤ 0/2 ∀` = 1, .'],\n",
       "       [152, '.'],\n",
       "       [153, '.'],\n",
       "       [154, ', L, (8) W̃LW̃L−1 · · · W̃1 = ŴLŴL−1 · · · Ŵ1.'],\n",
       "       [155,\n",
       "        '(9)\\nAny such perturbation also defines a local minimizer.'],\n",
       "       [156, 'Claim 1.'],\n",
       "       [157, 'Any tuple of matrices ( W̃1, .'],\n",
       "       [158, '.'],\n",
       "       [159, '.'],\n",
       "       [160,\n",
       "        ', W̃L ) satisfying (8) and (9) is necessarily a local minimizer F .'],\n",
       "       [161, 'Proof.'],\n",
       "       [162,\n",
       "        'For any matrixW` satisfying ‖W`−W̃`‖fro ≤ ε0/2, inequality (8) implies that\\n‖W` − Ŵ`‖fro ≤ ‖W` − W̃`‖fro + ‖W̃` − Ŵ`‖fro ≤ ε0\\nEquality (9) combined to (6) then leads to\\nF ( W1, .'],\n",
       "       [163, '.'],\n",
       "       [164, '.'],\n",
       "       [165,\n",
       "        ',WL ) ≥ f ( Â )\\n= f(ŴL · · · Ŵ1) = f(W̃L · · · W̃1) = F ( W̃1, .'],\n",
       "       [166, '.'],\n",
       "       [167, '.'],\n",
       "       [168,\n",
       "        ', W̃L ) for any W` with ‖W` − W̃`‖fro ≤ ε0/2 and so the point (W̃1, .'],\n",
       "       [169, '.'],\n",
       "       [170, '.'],\n",
       "       [171, ', W̃L) defines a local minimum.'],\n",
       "       [172,\n",
       "        'The construction of such perturbations requires a preliminary observation and then an appeal to the singular value decomposition.'],\n",
       "       [173,\n",
       "        'Due to the definition of Ŵk,− it follows that ker(Ŵk+1,−) = ker(Ŵk+1Ŵk,−) ⊇ ker(Ŵk,−), and so the chain of inclusions\\nker(Ŵ1,−) ⊆ ker(Ŵ2,−) ⊆ · · · ⊆ ker(ŴL−1,−) (10)\\nholds.'],\n",
       "       [174,\n",
       "        'Since ŴL−1,− has a nontrivial kernel, the chain of inclusions (10) implies that there exists k∗ ∈ {1, .'],\n",
       "       [175, '.'],\n",
       "       [176, '.'],\n",
       "       [177,\n",
       "        ', L−1} such that\\nker(Ŵk,−) = {0} if k < k∗ (11) ker(Ŵk,−) 6= {0} if k ≥ k∗ (12)\\nIn other words, Ŵk∗,− is the first matrix appearing in (10) that has a nontrivial kernel.'],\n",
       "       [178,\n",
       "        'The structural requirement (3) and the assumption that dL ≥ d0 imply that dk ≥ d0 for all k, and so the matrix Ŵk,− ∈ Mdk×d0 has more rows than columns.'],\n",
       "       [179,\n",
       "        'As a consequence its full singular value decomposition\\nŴk,− = ÛkΣ̂kV̂ T k (13)\\nhas the shape depicted in figure 1.'],\n",
       "       [180,\n",
       "        'The matrices Ûk ∈ Mdk×dk and V̂k ∈ Md0×d0 are orthogonal, whereas Σ̂k ∈ Mdk×d0 is a diagonal matrix containing the singular values of Ŵk,− in descending order.'],\n",
       "       [181,\n",
       "        'From (12) Ŵk,− has a nontrivial kernel for all k ≥ k∗, and so in particular its least singular value is zero.'],\n",
       "       [182,\n",
       "        'In particular, the (d0, d0) entry of Σ̂k vanishes if k ≥ k∗.'],\n",
       "       [183,\n",
       "        'Let ûk denote the corresponding dth0 column of Ûk, which exists since dk ≥ d0.'],\n",
       "       [184, 'Claim 2.'],\n",
       "       [185, 'Let wk∗+1, .'],\n",
       "       [186, '.'],\n",
       "       [187, '.'],\n",
       "       [188, ',wL denote any collection of vectors and δk∗+1, .'],\n",
       "       [189, '.'],\n",
       "       [190, '.'],\n",
       "       [191,\n",
       "        ', δL any collection of scalars satisfying\\nwk ∈ Rdk , ‖wk‖2 = 1 and (14) 0 ≤ δk ≤ 0/2 (15)\\nfor all k∗ + 1 ≤ k ≤ L. Then the tuple of matrices (W̃1, .'],\n",
       "       [192, '.'],\n",
       "       [193, '.'],\n",
       "       [194,\n",
       "        ', W̃L) defined by\\nW̃k := { Ŵk if 1 ≤ k ≤ k∗ Ŵk + δkwkû T k−1 else,\\n(16)\\nsatisfies (8) and (9).'],\n",
       "       [195, 'Proof.'],\n",
       "       [196,\n",
       "        'Inequality (8) follows from the fact that\\n‖W̃k − Ŵk‖fro = δk‖wkûTk−1‖fro = δk‖wk‖2‖ûk−1‖2\\nfor all k > k∗, together with the fact that ûk−1 and wk are unit vectors and that 0 ≤ δk ≤ 0/2.'],\n",
       "       [197,\n",
       "        'To prove (9) let W̃k,− = W̃k · · · W̃1 and Ŵk,− = Ŵk · · · Ŵ1 denote the truncated products of the matrices\\nW̃k and Ŵk.'],\n",
       "       [198,\n",
       "        'The equality W̃k∗,− = Ŵk∗,− is immediate from the definition (16).'],\n",
       "       [199,\n",
       "        'The equality (9) will then follow from showing that\\nW̃k,− = Ŵk,− for all k∗ ≤ k ≤ L. (17)\\nProceeding by induction, assume that W̃k,− = Ŵk,− for a given k ≥ k∗.'],\n",
       "       [200,\n",
       "        'Then\\nW̃k+1,− = W̃k+1W̃k,−\\n= W̃k+1Ŵk,− (induction hypothesis) = ( Ŵk+1 + δk+1wk+1û T k ) Ŵk,−\\n= Ŵk+1,− + δk+1wk+1u T k Ŵk,−\\nThe second term of the last line vanishes, since\\nuTk Ŵk,− = u T k ÛkΣ̂kV̂ T k = e T d0Σ̂kV̂ T k = 0\\nwith ed0 ∈ Rdk the dth0 standard basis vector.'],\n",
       "       [201,\n",
       "        'The second equality comes from the fact that the columns of Ûk are orthonormal, and the last equality comes from the fact that eTd0Σk∗ = 0 since the d th 0 row of Σ̂k∗ vanishes.'],\n",
       "       [202, 'Thus (17) holds, and so (9) holds as well.'],\n",
       "       [203, 'Claims 1 and claim 2 show that the perturbation( W̃1, .'],\n",
       "       [204, '.'],\n",
       "       [205, '.'],\n",
       "       [206, ', W̃L ) defined by (16) is a local minimizer of F .'],\n",
       "       [207,\n",
       "        'The critical point conditions\\n(i) 0 = W̃T2,+∇f ( Ã ) ,\\n(ii) 0 = W̃Tk+1,+∇f ( Ã ) W̃Tk−1,− ∀ 2 ≤ k ≤ L− 1,\\n(iii) 0 = ∇f ( Ã ) W̃TL−1,−\\ntherefore hold as well for all choices of wk∗+1, .'],\n",
       "       [208, '.'],\n",
       "       [209, '.'],\n",
       "       [210, ',wL and δk∗+1, .'],\n",
       "       [211, '.'],\n",
       "       [212, '.'],\n",
       "       [213, ', δL satisfying (14) and (15).'],\n",
       "       [214,\n",
       "        'The proof concludes by appealing to this family of critical point relations.'],\n",
       "       [215,\n",
       "        'If k∗ > 1 the transpose of condition (ii) gives\\nŴk∗−1,−∇f ( Â )T W̃k∗+1,+ = 0 (18)\\nsince the equalities W̃k∗−1,− = Ŵk∗−1,− (c.f.'],\n",
       "       [216, '(16)) and Ã = W̃L · · · W̃1 = ŴL · · · Ŵ1 = Â (c.f.'],\n",
       "       [217, '(9)) both hold.'],\n",
       "       [218, 'But ker(Ŵk∗−1,−) = {0} by definition of k∗ (c.f.'],\n",
       "       [219, '(11)), and so\\n∇f ( Â )T W̃L · · · W̃k∗+1 = 0.'],\n",
       "       [220, '(19)\\nmust hold as well.'],\n",
       "       [221,\n",
       "        'If k∗ = 1 then (19) follows trivially from the critical point condition (i).'],\n",
       "       [222, 'Thus (19) holds for all choices of wk∗+1, .'],\n",
       "       [223, '.'],\n",
       "       [224, '.'],\n",
       "       [225, ',wL and δk∗+1, .'],\n",
       "       [226, '.'],\n",
       "       [227, '.'],\n",
       "       [228, ', δL satisfying (14) and (15).'],\n",
       "       [229,\n",
       "        'First choose δk∗+1 = 0 so that W̃k∗+1 = Ŵk∗+1 and apply (19) to find\\n∇f ( Â )T W̃L · · · W̃k∗+2Ŵk∗+1 = 0.'],\n",
       "       [230,\n",
       "        '(20)\\nThen take any δk∗+1 > 0 and substract (20) from (19) to get\\n1 δk∗+1 ∇f ( Â )T W̃L · · · W̃k∗+2 ( W̃k∗+1 − Ŵk∗+1 ) = ∇f ( Â )T W̃L · · · W̃k∗+2 ( wk∗+1û T k∗ ) = 0\\nfor wk∗+1 an arbitrary vector with unit length.'],\n",
       "       [231,\n",
       "        'Right multiplying the last equality by ûk∗ and using the fact that (wk∗+1û T k∗ )ûk∗ = wk∗+1û T k∗ ûk∗ = wk∗+1 shows\\n∇f ( Â )T W̃L · · · W̃k∗+2wk∗+1 = 0 (21)\\nfor all choices of wk∗+1 with unit length.'],\n",
       "       [232,\n",
       "        'Thus (21) implies\\n∇f ( Â )T W̃L · · · W̃k∗+2 = 0\\nfor all choices of wk∗+2, .'],\n",
       "       [233, '.'],\n",
       "       [234, '.'],\n",
       "       [235, ',wL and δk∗+2, .'],\n",
       "       [236, '.'],\n",
       "       [237, '.'],\n",
       "       [238, ', δL satisfying (14) and (15).'],\n",
       "       [239, 'The claim\\n∇f ( Â ) = 0\\nthen follows by induction.'],\n",
       "       [240,\n",
       "        'Theorem 3 provides the mathematical basis for our analysis of deep linear problems.'],\n",
       "       [241, 'We therefore conclude by discussing its limits.'],\n",
       "       [242,\n",
       "        'First, theorem 3 fails if we refer to critical points rather than local minimizers.'],\n",
       "       [243,\n",
       "        'To see this, it suffices to observe that the critical point conditions for problem (P2),\\n(i) 0 = ŴT2,+∇f ( Â ) ,\\n(ii) 0 = ŴTk+1,+∇f ( Â ) ŴTk−1,− ∀ 2 ≤ k ≤ L− 1,\\n(iii) 0 = ∇f ( Â ) ŴTL−1,−\\nwhere Ŵk,+ := ŴL · · · Ŵk+1 and Ŵk,− := Ŵk−1 · · · Ŵ1, clearly hold if L ≥ 3 and all of the Ŵ` vanish.'],\n",
       "       [244,\n",
       "        'In other words, the collection of zero matrices always defines a critical point for (P2) but clearly ∇f ( 0 ) need not vanish.'],\n",
       "       [245,\n",
       "        'To\\nput it otherwise, if L ≥ 3 the problem (P2) always has saddle-points even though all local optima are global.'],\n",
       "       [246,\n",
       "        'Second, the assumption that f(A) is differentiable is necessary as well.'],\n",
       "       [247, 'More specifically, a function of the form\\nF (W1, .'],\n",
       "       [248, '.'],\n",
       "       [249, '.'],\n",
       "       [250,\n",
       "        ',WL) := f(WL · · ·W1)\\ncan have sub-optimal local minima if f(A) is convex and globally Lipschitz but is not differentiable.'],\n",
       "       [251,\n",
       "        'A simple example demonstrates this, and therefore proves theorem 2.'],\n",
       "       [252,\n",
       "        'For instance, consider the bi-variate convex function\\nf(x, y) := |x|+(1−y)+−1, (y)+ := max{y, 0}, (22)\\nwhich is clearly globally Lipschitz but not differentiable.'],\n",
       "       [253,\n",
       "        'The set\\narg min f := {(x, y) ∈ R2 : x = 0, y ≥ 1}\\nfurnishes its global minimizers while fopt = −1 gives the optimal value.'],\n",
       "       [254,\n",
       "        'For this function even a two layer deep linear problem\\nF ( W1,W2) := f(W2W1) W2 ∈ R2, W1 ∈ R\\nhas sub-optimal local minimizers; the point\\n(Ŵ1, Ŵ2) =\\n( 0, [ 1 0 ]) (23)\\nprovides an example of a sub-optimal solution.'],\n",
       "       [255,\n",
       "        'The set of all possible points in R2\\nN (Ŵ1, Ŵ2) :={ W2W1 : ‖W2 − Ŵ2‖ ≤ 1\\n4 , ‖W1 − Ŵ1‖ ≤\\n1\\n4 } generated by a 1/4-neighborhood of the optimum (23) lies in the two-sided, truncated cone\\nN (Ŵ1, Ŵ2) ⊂ {\\n(x, y) ∈ R2 : |x| ≤ 1 2 , |y| ≤ 1 2 |x| } ,\\nand so if we let x ∈ R denote the first component of the product W2W1 then the inequality\\nf(W2W1) ≥ 1\\n2 |x| ≥ 0 = f(Ŵ2Ŵ1)\\nholds on N (Ŵ1, Ŵ2) and so (Ŵ1, Ŵ2) is a sub-optimal local minimizer.'],\n",
       "       [256,\n",
       "        'Moreover, the minimizer (Ŵ1, Ŵ2) is a strict local minimizer in the only sense in which strict optimality can hold for a deep linear problem.'],\n",
       "       [257,\n",
       "        'Specifically, the strict inequality\\nf(W2W1) > f(Ŵ2Ŵ1) (24)\\nholds on N (Ŵ1, Ŵ2) unless W2W1 = Ŵ2Ŵ1 = 0; in the latter case (W1,W2) and (Ŵ1, Ŵ2) parametrize the same\\npoint and so their objectives must coincide.'],\n",
       "       [258, 'We may identify the underlying issue easily.'],\n",
       "       [259,\n",
       "        'The proof of theorem 3 requires a single-valued derivative ∇f(Â) at a local optimum, but with f(x, y) as in (22) its subdifferential\\n∂f(0) = {(x, y) ∈ R2 : −1 ≤ x ≤ 1, y = 0}\\nis multi-valued at the sub-optimal local minimum (23).'],\n",
       "       [260,\n",
       "        'In other words, if a globally convex function f(A) induces sub-optimal local minima in the corresponding deep linear problem then ∇f(Â) cannot exist at any such sub-optimal solution (assuming the structural condition, of course).'],\n",
       "       [261,\n",
       "        'Third, the structural hypothesis\\nd` ≥ min{dL, d0} for all ` ∈ {1, .'],\n",
       "       [262, '.'],\n",
       "       [263, '.'],\n",
       "       [264, ', L}\\nis necessary for theorem 3 to hold as well.'],\n",
       "       [265,\n",
       "        'If d` < min{d0, dL} for some ` the parametrization\\nA = WL · · ·W1\\ncannot recover full rank matrices.'],\n",
       "       [266,\n",
       "        'Let f(A) denote any function where∇f vanishes only at full rank matrices.'],\n",
       "       [267,\n",
       "        'Then\\n∇f ( WL · · ·W1 ) 6= 0\\nat all critical points of (P2), and so theorem 3 fails.'],\n",
       "       [268,\n",
       "        'Finally, if we do not require convexity of f(A) then it is not true, in general, that local minima of (P2) correspond to minima of the original problem.'],\n",
       "       [269,\n",
       "        'The functions\\nf(x, y) = x2 − y2 F (W1,W2) = f(W2W1)\\nand the minimizer (23) illustrate this point.'],\n",
       "       [270,\n",
       "        'While the origin is clearly a saddle point of the one layer problem, the argument leading to (24) shows that (23) is a local minimizer for the deep linear problem.'],\n",
       "       [271,\n",
       "        'So in the absence of additional structural assumptions on f(A), we may infer that a minimizer of the deep linear problem satisfies first-order optimality for the original problem, but nothing more.']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_sentences.query(\"paper_id == 223\")[[\"sentence\", \"text\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f73328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  'Deep linear networks (DLN) are neural networks that have multiple hidden layers but have no nonlinearities between layers.',\n",
       "  nan,\n",
       "  False],\n",
       " [1,\n",
       "  'That is, for given data points {x(i)}Ni=1 the outputs of such networks are computed via a series\\nŷ(i) = WLWL−1 · · ·W1x(i)\\nof matrix multiplications.',\n",
       "  nan,\n",
       "  False],\n",
       " [2,\n",
       "  'Given a target y(i) for the ith data point and a pairwise loss function `(ŷ(i),y(i)), forming the usual summation\\nL(W1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [3, '.', nan, False],\n",
       " [4, '.', nan, False],\n",
       " [5,\n",
       "  ',WL) = 1\\nN N∑ i=1 `(ŷ(i),y(i)) (1)\\nthen yields the total loss.',\n",
       "  nan,\n",
       "  False],\n",
       " [6,\n",
       "  'Such networks have few direct applications, but they frequently appear as a class of toy models used to understand the loss surfaces of deep neural networks (Saxe et al., 2014; Kawaguchi, 2016; Lu & Kawaguchi, 2017; Hardt & Ma, 2017).',\n",
       "  nan,\n",
       "  False],\n",
       " [7,\n",
       "  'For example, numerical experiments indicate that DLNs exhibit some behavior that resembles the behavior of\\n*Equal contribution 1Department of Mathematics, Loyola Marymount University, Los Angeles, CA 90045, USA 2Department of Mathematics and Statistics, California State University, Long Beach, Long Beach, CA 90840, USA.',\n",
       "  nan,\n",
       "  False],\n",
       " [8,\n",
       "  'Correspondence to: Thomas Laurent <tlaurent@lmu.edu>, James H. von Brecht <james.vonbrecht@csulb.edu>.',\n",
       "  nan,\n",
       "  False],\n",
       " [9,\n",
       "  'Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.',\n",
       "  nan,\n",
       "  False],\n",
       " [10, 'Copyright 2018 by the author(s).', nan, False],\n",
       " [11,\n",
       "  'deep nonlinear networks during training (Saxe et al., 2014).',\n",
       "  nan,\n",
       "  False],\n",
       " [12,\n",
       "  'Results of this sort provide a small piece of evidence that DLNs can provide a decent simplified model of more realistic networks with nonlinearities.',\n",
       "  nan,\n",
       "  False],\n",
       " [13,\n",
       "  'From an analytical point-of-view, the simplicity of DLNs allows for a rigorous, in-depth study of their loss surfaces.',\n",
       "  nan,\n",
       "  False],\n",
       " [14,\n",
       "  'These models typically employ a convex loss function `(ŷ,y), and so with one layer (i.e.',\n",
       "  nan,\n",
       "  False],\n",
       " [15,\n",
       "  'L = 1) the loss L(W1) is convex and the resulting optimization problem (1) has no sub-optimal local minimizers.',\n",
       "  nan,\n",
       "  False],\n",
       " [16, 'With multiple layers (i.e.', nan, False],\n",
       " [17, 'L ≥ 2) the loss L(W1, .', nan, False],\n",
       " [18, '.', nan, False],\n",
       " [19, '.', nan, False],\n",
       " [20,\n",
       "  ',WL) is not longer convex, and so the question of paramount interest concerns whether this addition of depth and the subsequent loss of convexity creates sub-optimal local minimizers.',\n",
       "  nan,\n",
       "  False],\n",
       " [21,\n",
       "  'Indeed, most analytical treatments of DLNs focus on this question.',\n",
       "  nan,\n",
       "  False],\n",
       " [22,\n",
       "  'We resolve this question in full for arbitrary convex differentiable loss functions.',\n",
       "  nan,\n",
       "  False],\n",
       " [23,\n",
       "  'Specifically, we consider deep linear networks satisfying the two following hypotheses:\\n(i) The loss function ŷ 7→ `(y, ŷ) is convex and differentiable.',\n",
       "  nan,\n",
       "  False],\n",
       " [24,\n",
       "  '(ii) The thinnest layer is either the input layer or the output layer.',\n",
       "  nan,\n",
       "  False],\n",
       " [25,\n",
       "  'Many networks of interest satisfy both of these hypotheses.',\n",
       "  nan,\n",
       "  False],\n",
       " [26,\n",
       "  'The first hypothesis (i) holds for nearly all network criteria, such as the mean squared error loss, the logistic loss or the cross entropy loss, that appear in applications.',\n",
       "  nan,\n",
       "  False],\n",
       " [27,\n",
       "  'In a classification scenario, the second hypothesis (ii) holds whenever each hidden layer has more neurons than the number of classes.',\n",
       "  nan,\n",
       "  False],\n",
       " [28,\n",
       "  'Thus both hypotheses are often satisfied when using a deep linear network (1) to model its nonlinear counterpart.',\n",
       "  nan,\n",
       "  False],\n",
       " [29,\n",
       "  'In any such situation we resolve the deep linear problem in its entirety.',\n",
       "  nan,\n",
       "  False],\n",
       " [30, 'Theorem 1.', nan, False],\n",
       " [31,\n",
       "  'If hypotheses (i) and (ii) hold then (1) has no sub-optimal minimizers, i.e.',\n",
       "  nan,\n",
       "  False],\n",
       " [32, 'any local minimum is global.', nan, False],\n",
       " [33, 'We provide a short, transparent proof of this result.', nan, False],\n",
       " [34,\n",
       "  'It is easily accessible to any reader with a basic understanding of the singular value decomposition, and in particular, it does not rely on any sophisticated machinery from either optimization or linear algebra.',\n",
       "  nan,\n",
       "  False],\n",
       " [35,\n",
       "  'Moreover, this theorem is the strongest possible in the following sense — Theorem 2.',\n",
       "  nan,\n",
       "  False],\n",
       " [36,\n",
       "  'There exists a convex, Lipschitz but not differentiable function ŷ 7→ `(y, ŷ) for which (1) has sub-optimal\\nlocal minimizers.',\n",
       "  nan,\n",
       "  False],\n",
       " [37,\n",
       "  'In other words, we have a (perhaps surprising) hard limit on how far “local equals global” results can reach; differentiability of the loss is essential.',\n",
       "  nan,\n",
       "  False],\n",
       " [38,\n",
       "  'Many prior analytical treatments of DLNs focus on similar questions.',\n",
       "  nan,\n",
       "  False],\n",
       " [39,\n",
       "  'For instance, both (Baldi & Hornik, 1989) and (Baldi & Lu, 2012) consider “deep” linear networks with two layers (i.e.',\n",
       "  nan,\n",
       "  False],\n",
       " [40, 'L = 2) and a mean squared error loss criterion.', nan, False],\n",
       " [41,\n",
       "  'They provide a “local equals global” result under some relatively mild assumptions on the data and targets.',\n",
       "  nan,\n",
       "  False],\n",
       " [42,\n",
       "  'More recently, (Kawaguchi, 2016) proved that deep linear networks with arbitrary number of layers and with mean squared error loss do not have sub-optimal local minima under certain structural assumptions on the data and targets.',\n",
       "  nan,\n",
       "  False],\n",
       " [43,\n",
       "  'The follow-up (Lu & Kawaguchi, 2017) futher simplifies the proof of this result and weakens the structural assumptions.',\n",
       "  nan,\n",
       "  False],\n",
       " [44,\n",
       "  'Specifically, this result shows that the loss (1) associated with a deep linear network has no sub-optimal local minima provided all of assumptions\\n(i) The loss `(ŷ(i),y(i)) = ‖y(i) − ŷ(i)‖2 is the mean squared error loss criterion;\\n(ii) The data matrixX = [x(1), .',\n",
       "  nan,\n",
       "  False],\n",
       " [45, '.', nan, False],\n",
       " [46, '.', nan, False],\n",
       " [47,\n",
       "  ',x(N)] has full row rank;\\n(iii) The target matrix Y = [y(1), .',\n",
       "  nan,\n",
       "  False],\n",
       " [48, '.', nan, False],\n",
       " [49, '.', nan, False],\n",
       " [50, ',y(N)] has full row rank;\\nare satisfied.', nan, False],\n",
       " [51,\n",
       "  'Compared to our result, (Lu & Kawaguchi, 2017) therefore allows for the hidden layers of the network to be thinner than the input and output layers.',\n",
       "  nan,\n",
       "  False],\n",
       " [52,\n",
       "  'However, our result applies to network equipped with any differentiable convex loss (in fact any differentiable loss L for which firstorder optimality implies global optimality) and we do not require any assumption on the data and targets.',\n",
       "  nan,\n",
       "  False],\n",
       " [53,\n",
       "  'Our proof is also shorter and much more elementary by comparison.',\n",
       "  '1',\n",
       "  True],\n",
       " [54,\n",
       "  'Theorem 1 follows as a simple consequence of a more general theorem concerning real-valued functions that take as input a product of matrices.',\n",
       "  nan,\n",
       "  False],\n",
       " [55,\n",
       "  'That is, we view the deep linear problem as a specific instance of the following more general problem.',\n",
       "  nan,\n",
       "  False],\n",
       " [56,\n",
       "  'Let Mm×n denote the space of m × n real matrices, and let f : MdL×d0 → R denote any differentiable function that takes dL × d0 matrices as input.',\n",
       "  nan,\n",
       "  False],\n",
       " [57,\n",
       "  'For any such function we may then consider both the single-layer optimization\\n(P1) { Minimize f(A) over all A in MdL×d0\\nas well as the analogous problem\\n(P2)  Minimize f(WLWL−1 · · ·W1) over all L-tuples (W1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [58, '.', nan, False],\n",
       " [59, '.', nan, False],\n",
       " [60,\n",
       "  ',WL) in Md1×d0 × · · · ×MdL×dL−1\\nthat corresponds to a multi-layer deep linear optimization.',\n",
       "  nan,\n",
       "  False],\n",
       " [61,\n",
       "  'In other words, in (P2) we consider the task of optimizing f over those matrices A ∈MdL×d0 that can be realized by an L-fold product\\nA = WLWL−1 · · ·W1 W` ∈Md`×d`−1 (2)\\nof matrices.',\n",
       "  '1',\n",
       "  True],\n",
       " [62,\n",
       "  'We may then ask how the parametrization (2) of A as a product of matrices affects the minimization of f, or in other words, whether the problems (P1) and (P2) have similar structure.',\n",
       "  nan,\n",
       "  False],\n",
       " [63,\n",
       "  'At heart, the use of DLNs to model nonlinear neural networks centers around this question.',\n",
       "  nan,\n",
       "  False],\n",
       " [64,\n",
       "  'Any notion of structural similarity between (P1) and (P2) should require that their global minima coincide.',\n",
       "  nan,\n",
       "  False],\n",
       " [65, 'As a matrix of the form (2) has rank at most min{d0, .', '1', True],\n",
       " [66, '.', nan, False],\n",
       " [67, '.', nan, False],\n",
       " [68,\n",
       "  ', dL}, we must impose the structural requirement\\nmin{d1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [69, '.', nan, False],\n",
       " [70, '.', nan, False],\n",
       " [71,\n",
       "  ', dL−1} ≥ min{dL, d0} (3)\\nin order to guarantee that (2) does, in fact, generate the full space of dL × d0 matrices.',\n",
       "  nan,\n",
       "  False],\n",
       " [72,\n",
       "  'Under this assumption we shall prove the following quite general theorem.',\n",
       "  nan,\n",
       "  False],\n",
       " [73, 'Theorem 3.', nan, False],\n",
       " [74,\n",
       "  'Assume that f(A) is any differentiable function and that the structural condition (3) holds.',\n",
       "  '1',\n",
       "  True],\n",
       " [75, 'Then at any local minimizer ( Ŵ1, .', '1', True],\n",
       " [76, '.', nan, False],\n",
       " [77, '.', nan, False],\n",
       " [78,\n",
       "  ', ŴL ) of (P2) the optimality condition\\n∇f ( Â ) = 0 Â := ŴLŴL−1 · · · Ŵ1\\nis satisfied.',\n",
       "  nan,\n",
       "  False],\n",
       " [79,\n",
       "  'Theorem 1 follows as a simple consequence of this theorem.',\n",
       "  nan,\n",
       "  False],\n",
       " [80,\n",
       "  'The first hypothesis (i) of theorem 1 shows that the total loss (1) takes the form\\nL(W1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [81, '.', nan, False],\n",
       " [82, '.', nan, False],\n",
       " [83,\n",
       "  ',WL) = f(WL · · ·W1)\\nfor f(A) some convex and differentiable function.',\n",
       "  '1',\n",
       "  True],\n",
       " [84,\n",
       "  'The structural hypothesis (3) is equivalent to the second hypothesis (ii) of theorem 1, and so we can directly apply theorem 3 to conclude that a local minimum ( Ŵ1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [85, '.', nan, False],\n",
       " [86, '.', nan, False],\n",
       " [87,\n",
       "  ', ŴL ) of L corresponds to a critical point Â = ŴL · · · Ŵ1 of f(A), and since f(A) is convex, this critical point is necessarily a global minimum.',\n",
       "  nan,\n",
       "  False],\n",
       " [88,\n",
       "  'Before turning to the proof of theorem 3 we recall a bit of notation and provide a calculus lemma.',\n",
       "  nan,\n",
       "  False],\n",
       " [89,\n",
       "  'Let\\n〈A,B〉fro := Tr(ATB) = ∑ i ∑ j AijBij and\\n‖A‖2fro := 〈A,A〉fro\\ndenote the Frobenius dot product and the Frobenius norm, respectively.',\n",
       "  nan,\n",
       "  False],\n",
       " [90,\n",
       "  'Also, recall that for a differentiable function φ : Mm×n 7→ R its gradient∇φ(A) ∈Mm×n is the unique matrix so that the equality\\nφ(A+H) = φ(A) + 〈∇φ(A), H〉fro + o (‖H‖fro) (4)\\nholds.',\n",
       "  nan,\n",
       "  False],\n",
       " [91, 'If F (W1, .', nan, False],\n",
       " [92, '.', nan, False],\n",
       " [93, '.', nan, False],\n",
       " [94,\n",
       "  ',WL) := f(WL · · ·W1) denotes the objective of interest in (P2) the following lemma gives the partial derivatives of F as a function of its arguments.',\n",
       "  nan,\n",
       "  False],\n",
       " [95, 'Lemma 1.', nan, False],\n",
       " [96, 'The partial derivatives of F are given by\\n∇W1F (W1, .', nan, False],\n",
       " [97, '.', nan, False],\n",
       " [98, '.', nan, False],\n",
       " [99, ',WL) = WT2,+∇f ( A ) ,\\n∇WkF (W1, .', nan, False],\n",
       " [100, '.', nan, False],\n",
       " [101, '.', nan, False],\n",
       " [102, ',WL) = WTk+1,+∇f(A)WTk−1,−, ∇WLF (W1, .', nan, False],\n",
       " [103, '.', nan, False],\n",
       " [104, '.', nan, False],\n",
       " [105,\n",
       "  ',WL) = ∇f ( A ) WTL−1,−,\\nwhere A stands for the full product A := WL · · ·W1 and Wk,+,Wk,− are the truncated products\\nWk,+ := WL · · ·Wk, Wk,− := Wk · · ·W1.',\n",
       "  nan,\n",
       "  False],\n",
       " [106, '(5)\\nProof.', nan, False],\n",
       " [107, 'The definition (4) implies\\nF (W1, .', nan, False],\n",
       " [108, '.', nan, False],\n",
       " [109, '.', nan, False],\n",
       " [110, ',Wk−1,Wk +H,Wk+1, .', nan, False],\n",
       " [111, '.', nan, False],\n",
       " [112, '.', nan, False],\n",
       " [113,\n",
       "  ',WL) = f ( A+Wk+1,+HWk−1,− ) = f(A) + 〈∇f(A),Wk+1,+HWk−1,−〉fro + o ( ‖H‖fro ) .',\n",
       "  nan,\n",
       "  False],\n",
       " [114,\n",
       "  'Using the cyclic property Tr(ABC) = Tr(CAB) of the trace then gives\\n〈∇f(A) , Wk+1,+HWk−1,− 〉fro = Tr ( ∇f(A)TWk+1,+HWk−1,− ) = Tr ( Wk−1,−∇f(A)TWk+1,+H\\n) = 〈WTk+1,+∇f(A)WTk−1,− , H 〉fro\\nwhich, in light of (4), gives the desired formula for∇WkF .',\n",
       "  '1',\n",
       "  True],\n",
       " [115, 'The formulas for∇W1F and∇WLF are obtained similarly.', '1', True],\n",
       " [116,\n",
       "  'Proof of Theorem 3: To prove theorem 3 it suffices to assume that dL ≥ d0 without loss of generality.',\n",
       "  nan,\n",
       "  False],\n",
       " [117,\n",
       "  'This follows from the simple observation that\\ng ( A ) := f ( AT )\\ndefines a differentiable function of d0 × dL matrices for f(A) any differentiable function of dL × d0 matrices.',\n",
       "  nan,\n",
       "  False],\n",
       " [118, 'As a point ( W1, .', nan, False],\n",
       " [119, '.', nan, False],\n",
       " [120, '.', nan, False],\n",
       " [121,\n",
       "  ',WL ) defines a local minimum of\\nf ( WLWL−1 · · ·W1 ) if and only if ( WT1 , .',\n",
       "  nan,\n",
       "  False],\n",
       " [122, '.', nan, False],\n",
       " [123, '.', nan, False],\n",
       " [124,\n",
       "  ',W T L ) defines\\na minimum of g ( V1 · · ·VL−1VL ) , the theorem for the case dL < d0 follows by appealing to its dL ≥ d0 instance.',\n",
       "  nan,\n",
       "  False],\n",
       " [125,\n",
       "  'It\\ntherefore suffices to assume that dL ≥ d0, and by the structural assumption that dk ≥ d0, throughout the remainder of the proof.',\n",
       "  nan,\n",
       "  False],\n",
       " [126, 'Consider any local minimizer ( Ŵ1, .', nan, False],\n",
       " [127, '.', nan, False],\n",
       " [128, '.', nan, False],\n",
       " [129,\n",
       "  ', ŴL ) of F and denote by Â, Ŵk,+ and Ŵk,− the corresponding full and truncated products (c.f.',\n",
       "  nan,\n",
       "  False],\n",
       " [130, '(5)).', nan, False],\n",
       " [131,\n",
       "  'By definition of a local minimizer there exists some ε0 > 0 so that\\nF (W1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [132, '.', nan, False],\n",
       " [133, '.', nan, False],\n",
       " [134, ',WL) ≥ F (Ŵ1, .', nan, False],\n",
       " [135, '.', nan, False],\n",
       " [136, '.', nan, False],\n",
       " [137,\n",
       "  ', ŴL) = f ( Â ) (6)\\nwhenever the family of inequalities\\n‖W` − Ŵ`‖fro ≤ ε0 for all 1 ≤ ` ≤ L\\nall hold.',\n",
       "  nan,\n",
       "  False],\n",
       " [138,\n",
       "  'Moreover, lemma 1 yields (i) 0 = ŴT2,+∇f ( Â ) ,\\n(ii) 0 = ŴTk+1,+∇f ( Â ) ŴTk−1,− ∀ 2 ≤ k ≤ L− 1,\\n(iii) 0 = ∇f ( Â ) ŴTL−1,−.',\n",
       "  nan,\n",
       "  False],\n",
       " [139,\n",
       "  '(7)\\nsince all partial derivatives must vanish at a local minimum.',\n",
       "  nan,\n",
       "  False],\n",
       " [140, 'If ŴL−1,− has a trivial kernel, i.e.', nan, False],\n",
       " [141, 'ker(ŴL−1,−) = {0}, then the theorem follows easily.', nan, False],\n",
       " [142,\n",
       "  'The critical point condition (7) part (iii) implies\\nŴL−1,−∇f ( Â )T = 0,\\nand since ŴL−1,− has a trivial kernel this implies ∇f ( Â ) = ∇f ( ŴLŴL−1 · · · Ŵ1 ) = 0 as desired.',\n",
       "  '1',\n",
       "  True],\n",
       " [143,\n",
       "  'The remainder of the proof concerns the case that ŴL−1,− has a nontrivial kernel.',\n",
       "  '1',\n",
       "  True],\n",
       " [144,\n",
       "  'The main idea is to use this nontrivial kernel to construct a family of infinitesimal perturbations of the local minimizer ( Ŵ1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [145, '.', nan, False],\n",
       " [146, '.', nan, False],\n",
       " [147,\n",
       "  ', ŴL ) that leaves the overall product ŴL · · · Ŵ1 unchanged.',\n",
       "  nan,\n",
       "  False],\n",
       " [148, 'In other words, the family of perturbations ( W̃1, .', nan, False],\n",
       " [149, '.', nan, False],\n",
       " [150, '.', nan, False],\n",
       " [151, ', W̃L ) satisfy\\n‖W̃` − Ŵ`‖fro ≤ 0/2 ∀` = 1, .', nan, False],\n",
       " [152, '.', nan, False],\n",
       " [153, '.', nan, False],\n",
       " [154, ', L, (8) W̃LW̃L−1 · · · W̃1 = ŴLŴL−1 · · · Ŵ1.', nan, False],\n",
       " [155,\n",
       "  '(9)\\nAny such perturbation also defines a local minimizer.',\n",
       "  '1',\n",
       "  True],\n",
       " [156, 'Claim 1.', nan, False],\n",
       " [157, 'Any tuple of matrices ( W̃1, .', '1', True],\n",
       " [158, '.', nan, False],\n",
       " [159, '.', nan, False],\n",
       " [160,\n",
       "  ', W̃L ) satisfying (8) and (9) is necessarily a local minimizer F .',\n",
       "  nan,\n",
       "  False],\n",
       " [161, 'Proof.', nan, False],\n",
       " [162,\n",
       "  'For any matrixW` satisfying ‖W`−W̃`‖fro ≤ ε0/2, inequality (8) implies that\\n‖W` − Ŵ`‖fro ≤ ‖W` − W̃`‖fro + ‖W̃` − Ŵ`‖fro ≤ ε0\\nEquality (9) combined to (6) then leads to\\nF ( W1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [163, '.', nan, False],\n",
       " [164, '.', nan, False],\n",
       " [165,\n",
       "  ',WL ) ≥ f ( Â )\\n= f(ŴL · · · Ŵ1) = f(W̃L · · · W̃1) = F ( W̃1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [166, '.', nan, False],\n",
       " [167, '.', nan, False],\n",
       " [168,\n",
       "  ', W̃L ) for any W` with ‖W` − W̃`‖fro ≤ ε0/2 and so the point (W̃1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [169, '.', nan, False],\n",
       " [170, '.', nan, False],\n",
       " [171, ', W̃L) defines a local minimum.', nan, False],\n",
       " [172,\n",
       "  'The construction of such perturbations requires a preliminary observation and then an appeal to the singular value decomposition.',\n",
       "  nan,\n",
       "  False],\n",
       " [173,\n",
       "  'Due to the definition of Ŵk,− it follows that ker(Ŵk+1,−) = ker(Ŵk+1Ŵk,−) ⊇ ker(Ŵk,−), and so the chain of inclusions\\nker(Ŵ1,−) ⊆ ker(Ŵ2,−) ⊆ · · · ⊆ ker(ŴL−1,−) (10)\\nholds.',\n",
       "  nan,\n",
       "  False],\n",
       " [174,\n",
       "  'Since ŴL−1,− has a nontrivial kernel, the chain of inclusions (10) implies that there exists k∗ ∈ {1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [175, '.', nan, False],\n",
       " [176, '.', nan, False],\n",
       " [177,\n",
       "  ', L−1} such that\\nker(Ŵk,−) = {0} if k < k∗ (11) ker(Ŵk,−) 6= {0} if k ≥ k∗ (12)\\nIn other words, Ŵk∗,− is the first matrix appearing in (10) that has a nontrivial kernel.',\n",
       "  nan,\n",
       "  False],\n",
       " [178,\n",
       "  'The structural requirement (3) and the assumption that dL ≥ d0 imply that dk ≥ d0 for all k, and so the matrix Ŵk,− ∈ Mdk×d0 has more rows than columns.',\n",
       "  nan,\n",
       "  False],\n",
       " [179,\n",
       "  'As a consequence its full singular value decomposition\\nŴk,− = ÛkΣ̂kV̂ T k (13)\\nhas the shape depicted in figure 1.',\n",
       "  nan,\n",
       "  False],\n",
       " [180,\n",
       "  'The matrices Ûk ∈ Mdk×dk and V̂k ∈ Md0×d0 are orthogonal, whereas Σ̂k ∈ Mdk×d0 is a diagonal matrix containing the singular values of Ŵk,− in descending order.',\n",
       "  nan,\n",
       "  False],\n",
       " [181,\n",
       "  'From (12) Ŵk,− has a nontrivial kernel for all k ≥ k∗, and so in particular its least singular value is zero.',\n",
       "  nan,\n",
       "  False],\n",
       " [182,\n",
       "  'In particular, the (d0, d0) entry of Σ̂k vanishes if k ≥ k∗.',\n",
       "  nan,\n",
       "  False],\n",
       " [183,\n",
       "  'Let ûk denote the corresponding dth0 column of Ûk, which exists since dk ≥ d0.',\n",
       "  '1',\n",
       "  True],\n",
       " [184, 'Claim 2.', nan, False],\n",
       " [185, 'Let wk∗+1, .', nan, False],\n",
       " [186, '.', nan, False],\n",
       " [187, '.', nan, False],\n",
       " [188, ',wL denote any collection of vectors and δk∗+1, .', nan, False],\n",
       " [189, '.', nan, False],\n",
       " [190, '.', nan, False],\n",
       " [191,\n",
       "  ', δL any collection of scalars satisfying\\nwk ∈ Rdk , ‖wk‖2 = 1 and (14) 0 ≤ δk ≤ 0/2 (15)\\nfor all k∗ + 1 ≤ k ≤ L. Then the tuple of matrices (W̃1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [192, '.', nan, False],\n",
       " [193, '.', nan, False],\n",
       " [194,\n",
       "  ', W̃L) defined by\\nW̃k := { Ŵk if 1 ≤ k ≤ k∗ Ŵk + δkwkû T k−1 else,\\n(16)\\nsatisfies (8) and (9).',\n",
       "  nan,\n",
       "  False],\n",
       " [195, 'Proof.', nan, False],\n",
       " [196,\n",
       "  'Inequality (8) follows from the fact that\\n‖W̃k − Ŵk‖fro = δk‖wkûTk−1‖fro = δk‖wk‖2‖ûk−1‖2\\nfor all k > k∗, together with the fact that ûk−1 and wk are unit vectors and that 0 ≤ δk ≤ 0/2.',\n",
       "  nan,\n",
       "  False],\n",
       " [197,\n",
       "  'To prove (9) let W̃k,− = W̃k · · · W̃1 and Ŵk,− = Ŵk · · · Ŵ1 denote the truncated products of the matrices\\nW̃k and Ŵk.',\n",
       "  '1',\n",
       "  True],\n",
       " [198,\n",
       "  'The equality W̃k∗,− = Ŵk∗,− is immediate from the definition (16).',\n",
       "  nan,\n",
       "  False],\n",
       " [199,\n",
       "  'The equality (9) will then follow from showing that\\nW̃k,− = Ŵk,− for all k∗ ≤ k ≤ L. (17)\\nProceeding by induction, assume that W̃k,− = Ŵk,− for a given k ≥ k∗.',\n",
       "  nan,\n",
       "  False],\n",
       " [200,\n",
       "  'Then\\nW̃k+1,− = W̃k+1W̃k,−\\n= W̃k+1Ŵk,− (induction hypothesis) = ( Ŵk+1 + δk+1wk+1û T k ) Ŵk,−\\n= Ŵk+1,− + δk+1wk+1u T k Ŵk,−\\nThe second term of the last line vanishes, since\\nuTk Ŵk,− = u T k ÛkΣ̂kV̂ T k = e T d0Σ̂kV̂ T k = 0\\nwith ed0 ∈ Rdk the dth0 standard basis vector.',\n",
       "  nan,\n",
       "  False],\n",
       " [201,\n",
       "  'The second equality comes from the fact that the columns of Ûk are orthonormal, and the last equality comes from the fact that eTd0Σk∗ = 0 since the d th 0 row of Σ̂k∗ vanishes.',\n",
       "  nan,\n",
       "  False],\n",
       " [202, 'Thus (17) holds, and so (9) holds as well.', nan, False],\n",
       " [203, 'Claims 1 and claim 2 show that the perturbation( W̃1, .', nan, False],\n",
       " [204, '.', nan, False],\n",
       " [205, '.', nan, False],\n",
       " [206, ', W̃L ) defined by (16) is a local minimizer of F .', nan, False],\n",
       " [207,\n",
       "  'The critical point conditions\\n(i) 0 = W̃T2,+∇f ( Ã ) ,\\n(ii) 0 = W̃Tk+1,+∇f ( Ã ) W̃Tk−1,− ∀ 2 ≤ k ≤ L− 1,\\n(iii) 0 = ∇f ( Ã ) W̃TL−1,−\\ntherefore hold as well for all choices of wk∗+1, .',\n",
       "  '1',\n",
       "  True],\n",
       " [208, '.', nan, False],\n",
       " [209, '.', nan, False],\n",
       " [210, ',wL and δk∗+1, .', nan, False],\n",
       " [211, '.', nan, False],\n",
       " [212, '.', nan, False],\n",
       " [213, ', δL satisfying (14) and (15).', '1', True],\n",
       " [214,\n",
       "  'The proof concludes by appealing to this family of critical point relations.',\n",
       "  nan,\n",
       "  False],\n",
       " [215,\n",
       "  'If k∗ > 1 the transpose of condition (ii) gives\\nŴk∗−1,−∇f ( Â )T W̃k∗+1,+ = 0 (18)\\nsince the equalities W̃k∗−1,− = Ŵk∗−1,− (c.f.',\n",
       "  nan,\n",
       "  False],\n",
       " [216, '(16)) and Ã = W̃L · · · W̃1 = ŴL · · · Ŵ1 = Â (c.f.', nan, False],\n",
       " [217, '(9)) both hold.', nan, False],\n",
       " [218, 'But ker(Ŵk∗−1,−) = {0} by definition of k∗ (c.f.', nan, False],\n",
       " [219, '(11)), and so\\n∇f ( Â )T W̃L · · · W̃k∗+1 = 0.', nan, False],\n",
       " [220, '(19)\\nmust hold as well.', nan, False],\n",
       " [221,\n",
       "  'If k∗ = 1 then (19) follows trivially from the critical point condition (i).',\n",
       "  nan,\n",
       "  False],\n",
       " [222, 'Thus (19) holds for all choices of wk∗+1, .', nan, False],\n",
       " [223, '.', nan, False],\n",
       " [224, '.', nan, False],\n",
       " [225, ',wL and δk∗+1, .', nan, False],\n",
       " [226, '.', nan, False],\n",
       " [227, '.', nan, False],\n",
       " [228, ', δL satisfying (14) and (15).', '1', True],\n",
       " [229,\n",
       "  'First choose δk∗+1 = 0 so that W̃k∗+1 = Ŵk∗+1 and apply (19) to find\\n∇f ( Â )T W̃L · · · W̃k∗+2Ŵk∗+1 = 0.',\n",
       "  nan,\n",
       "  False],\n",
       " [230,\n",
       "  '(20)\\nThen take any δk∗+1 > 0 and substract (20) from (19) to get\\n1 δk∗+1 ∇f ( Â )T W̃L · · · W̃k∗+2 ( W̃k∗+1 − Ŵk∗+1 ) = ∇f ( Â )T W̃L · · · W̃k∗+2 ( wk∗+1û T k∗ ) = 0\\nfor wk∗+1 an arbitrary vector with unit length.',\n",
       "  nan,\n",
       "  False],\n",
       " [231,\n",
       "  'Right multiplying the last equality by ûk∗ and using the fact that (wk∗+1û T k∗ )ûk∗ = wk∗+1û T k∗ ûk∗ = wk∗+1 shows\\n∇f ( Â )T W̃L · · · W̃k∗+2wk∗+1 = 0 (21)\\nfor all choices of wk∗+1 with unit length.',\n",
       "  '1',\n",
       "  True],\n",
       " [232,\n",
       "  'Thus (21) implies\\n∇f ( Â )T W̃L · · · W̃k∗+2 = 0\\nfor all choices of wk∗+2, .',\n",
       "  '1',\n",
       "  True],\n",
       " [233, '.', nan, False],\n",
       " [234, '.', nan, False],\n",
       " [235, ',wL and δk∗+2, .', nan, False],\n",
       " [236, '.', nan, False],\n",
       " [237, '.', nan, False],\n",
       " [238, ', δL satisfying (14) and (15).', '1', True],\n",
       " [239, 'The claim\\n∇f ( Â ) = 0\\nthen follows by induction.', nan, False],\n",
       " [240,\n",
       "  'Theorem 3 provides the mathematical basis for our analysis of deep linear problems.',\n",
       "  nan,\n",
       "  False],\n",
       " [241, 'We therefore conclude by discussing its limits.', nan, False],\n",
       " [242,\n",
       "  'First, theorem 3 fails if we refer to critical points rather than local minimizers.',\n",
       "  '1',\n",
       "  True],\n",
       " [243,\n",
       "  'To see this, it suffices to observe that the critical point conditions for problem (P2),\\n(i) 0 = ŴT2,+∇f ( Â ) ,\\n(ii) 0 = ŴTk+1,+∇f ( Â ) ŴTk−1,− ∀ 2 ≤ k ≤ L− 1,\\n(iii) 0 = ∇f ( Â ) ŴTL−1,−\\nwhere Ŵk,+ := ŴL · · · Ŵk+1 and Ŵk,− := Ŵk−1 · · · Ŵ1, clearly hold if L ≥ 3 and all of the Ŵ` vanish.',\n",
       "  nan,\n",
       "  False],\n",
       " [244,\n",
       "  'In other words, the collection of zero matrices always defines a critical point for (P2) but clearly ∇f ( 0 ) need not vanish.',\n",
       "  nan,\n",
       "  False],\n",
       " [245,\n",
       "  'To\\nput it otherwise, if L ≥ 3 the problem (P2) always has saddle-points even though all local optima are global.',\n",
       "  nan,\n",
       "  False],\n",
       " [246,\n",
       "  'Second, the assumption that f(A) is differentiable is necessary as well.',\n",
       "  nan,\n",
       "  False],\n",
       " [247, 'More specifically, a function of the form\\nF (W1, .', nan, False],\n",
       " [248, '.', nan, False],\n",
       " [249, '.', nan, False],\n",
       " [250,\n",
       "  ',WL) := f(WL · · ·W1)\\ncan have sub-optimal local minima if f(A) is convex and globally Lipschitz but is not differentiable.',\n",
       "  nan,\n",
       "  False],\n",
       " [251,\n",
       "  'A simple example demonstrates this, and therefore proves theorem 2.',\n",
       "  nan,\n",
       "  False],\n",
       " [252,\n",
       "  'For instance, consider the bi-variate convex function\\nf(x, y) := |x|+(1−y)+−1, (y)+ := max{y, 0}, (22)\\nwhich is clearly globally Lipschitz but not differentiable.',\n",
       "  nan,\n",
       "  False],\n",
       " [253,\n",
       "  'The set\\narg min f := {(x, y) ∈ R2 : x = 0, y ≥ 1}\\nfurnishes its global minimizers while fopt = −1 gives the optimal value.',\n",
       "  nan,\n",
       "  False],\n",
       " [254,\n",
       "  'For this function even a two layer deep linear problem\\nF ( W1,W2) := f(W2W1) W2 ∈ R2, W1 ∈ R\\nhas sub-optimal local minimizers; the point\\n(Ŵ1, Ŵ2) =\\n( 0, [ 1 0 ]) (23)\\nprovides an example of a sub-optimal solution.',\n",
       "  '1',\n",
       "  True],\n",
       " [255,\n",
       "  'The set of all possible points in R2\\nN (Ŵ1, Ŵ2) :={ W2W1 : ‖W2 − Ŵ2‖ ≤ 1\\n4 , ‖W1 − Ŵ1‖ ≤\\n1\\n4 } generated by a 1/4-neighborhood of the optimum (23) lies in the two-sided, truncated cone\\nN (Ŵ1, Ŵ2) ⊂ {\\n(x, y) ∈ R2 : |x| ≤ 1 2 , |y| ≤ 1 2 |x| } ,\\nand so if we let x ∈ R denote the first component of the product W2W1 then the inequality\\nf(W2W1) ≥ 1\\n2 |x| ≥ 0 = f(Ŵ2Ŵ1)\\nholds on N (Ŵ1, Ŵ2) and so (Ŵ1, Ŵ2) is a sub-optimal local minimizer.',\n",
       "  nan,\n",
       "  False],\n",
       " [256,\n",
       "  'Moreover, the minimizer (Ŵ1, Ŵ2) is a strict local minimizer in the only sense in which strict optimality can hold for a deep linear problem.',\n",
       "  nan,\n",
       "  False],\n",
       " [257,\n",
       "  'Specifically, the strict inequality\\nf(W2W1) > f(Ŵ2Ŵ1) (24)\\nholds on N (Ŵ1, Ŵ2) unless W2W1 = Ŵ2Ŵ1 = 0; in the latter case (W1,W2) and (Ŵ1, Ŵ2) parametrize the same\\npoint and so their objectives must coincide.',\n",
       "  nan,\n",
       "  False],\n",
       " [258, 'We may identify the underlying issue easily.', nan, False],\n",
       " [259,\n",
       "  'The proof of theorem 3 requires a single-valued derivative ∇f(Â) at a local optimum, but with f(x, y) as in (22) its subdifferential\\n∂f(0) = {(x, y) ∈ R2 : −1 ≤ x ≤ 1, y = 0}\\nis multi-valued at the sub-optimal local minimum (23).',\n",
       "  '1',\n",
       "  True],\n",
       " [260,\n",
       "  'In other words, if a globally convex function f(A) induces sub-optimal local minima in the corresponding deep linear problem then ∇f(Â) cannot exist at any such sub-optimal solution (assuming the structural condition, of course).',\n",
       "  nan,\n",
       "  False],\n",
       " [261,\n",
       "  'Third, the structural hypothesis\\nd` ≥ min{dL, d0} for all ` ∈ {1, .',\n",
       "  nan,\n",
       "  False],\n",
       " [262, '.', nan, False],\n",
       " [263, '.', nan, False],\n",
       " [264, ', L}\\nis necessary for theorem 3 to hold as well.', '1', True],\n",
       " [265,\n",
       "  'If d` < min{d0, dL} for some ` the parametrization\\nA = WL · · ·W1\\ncannot recover full rank matrices.',\n",
       "  nan,\n",
       "  False],\n",
       " [266,\n",
       "  'Let f(A) denote any function where∇f vanishes only at full rank matrices.',\n",
       "  nan,\n",
       "  False],\n",
       " [267,\n",
       "  'Then\\n∇f ( WL · · ·W1 ) 6= 0\\nat all critical points of (P2), and so theorem 3 fails.',\n",
       "  '1',\n",
       "  True],\n",
       " [268,\n",
       "  'Finally, if we do not require convexity of f(A) then it is not true, in general, that local minima of (P2) correspond to minima of the original problem.',\n",
       "  '1',\n",
       "  True],\n",
       " [269,\n",
       "  'The functions\\nf(x, y) = x2 − y2 F (W1,W2) = f(W2W1)\\nand the minimizer (23) illustrate this point.',\n",
       "  '8',\n",
       "  True],\n",
       " [270,\n",
       "  'While the origin is clearly a saddle point of the one layer problem, the argument leading to (24) shows that (23) is a local minimizer for the deep linear problem.',\n",
       "  '61',\n",
       "  True],\n",
       " [271,\n",
       "  'So in the absence of additional structural assumptions on f(A), we may infer that a minimizer of the deep linear problem satisfies first-order optimality for the original problem, but nothing more.',\n",
       "  '61',\n",
       "  True]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.query(\"paper_id == 223\").drop(columns = \"paper_id\").values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c033aff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43773, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.query(\"in_summary == True\").shape\n",
    "# (37749, 5) exact match\n",
    "# (43787, 5) remove non-word characters and make it lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bdb4203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_id\n",
       "825     32\n",
       "1496    32\n",
       "1498    32\n",
       "1278    31\n",
       "745     31\n",
       "1012    31\n",
       "133     31\n",
       "1554    31\n",
       "434     31\n",
       "691     31\n",
       "346     31\n",
       "1647    31\n",
       "556     31\n",
       "195     30\n",
       "974     30\n",
       "939     30\n",
       "964     30\n",
       "1239    30\n",
       "293     30\n",
       "290     30\n",
       "Name: in_summary, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.groupby(\"paper_id\").in_summary.sum().sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d8a72d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels.groupby(\"paper_id\").in_summary.sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344a153",
   "metadata": {},
   "source": [
    "# writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0913db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.to_pickle(\"labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecfbf3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.to_pickle(\"papers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2ea24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries.to_pickle(\"summaries.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a4f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
