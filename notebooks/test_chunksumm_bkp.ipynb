{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "# os.environ['TRANSFORMERS_OFFLINE'] = '1' # Indicating transformers for offline mode\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "from transformers.models.auto.tokenization_auto import logger\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "TRANSFORMER_CACHE = Path(\"../resources/transformer_cache\") # The cache location. Accessed when offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHUNKSUMM(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=6e-5, n_classes=2, enable_chunk=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = model\n",
    "\n",
    "        # Freezing bert params\n",
    "        # for param in self.bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # self.bert.eval()\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l1 = torch.nn.Linear(768, n_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        self.auc = torchmetrics.AUROC(num_classes=n_classes)\n",
    "        self.enable_chunk = enable_chunk\n",
    "       \n",
    "        # self.save_hyperparameters() # Saves every *args in _init_() in checkpoint file. # Slows trainer.predict\n",
    "        self.save_hyperparameters(ignore=[\"bert\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, train=False):\n",
    "        \"\"\"Can handle more than 512 tokens\"\"\"\n",
    "\n",
    "        embed2d = self.get_embedding(input_ids, attention_mask, token_type_ids)\n",
    "        logits = self.l1(embed2d)  \n",
    "        if train:\n",
    "            return logits\n",
    "        else: \n",
    "            return torch.softmax(logits,dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_embedding(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        if self.enable_chunk:\n",
    "\n",
    "            batch_chunks = [\n",
    "                self.chunk(batch) for batch in (input_ids, attention_mask, token_type_ids)\n",
    "            ]\n",
    "            handler = []\n",
    "            for chunk in zip(batch_chunks[0], batch_chunks[1], batch_chunks[2]):\n",
    "\n",
    "                chunk_hidden_states = self.bert(chunk[0], chunk[1], chunk[2], output_hidden_states=True)[2]\n",
    "                chunk_embed2d = torch.stack(chunk_hidden_states)[-5:].mean(0)\n",
    "                handler.append(chunk_embed2d)\n",
    "\n",
    "            contextual_encoding = torch.cat(handler, dim=1)\n",
    "            embed2d = contextual_encoding\n",
    "\n",
    "        else:\n",
    "            hidden_states = self.bert(input_ids[:, :512], attention_mask[:, :512], token_type_ids[:, :512], output_hidden_states=True)[2]\n",
    "            mean_hidden_states = torch.stack(hidden_states)[-5:].mean(0)\n",
    "            contextual_encoding = mean_hidden_states\n",
    "            embed2d = contextual_encoding\n",
    "\n",
    "        return embed2d\n",
    "\n",
    "    def training_step(self, batch, batch_ids=None):\n",
    "\n",
    "\n",
    "\n",
    "        outputs = self(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"],train=True)\n",
    "\n",
    "        labels = self.expand_targets(batch[\"targets\"].float()) \n",
    "        labels = labels.reshape_as(outputs)\n",
    "\n",
    "\n",
    "        loss = self.criterion(outputs, labels) \n",
    "        #acc = self.accuracy(outputs, labels.int())\n",
    "        auc = self.auc(outputs, labels.int())\n",
    "\n",
    "\n",
    "        \n",
    "        self.log(\"Loss_train\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"Auc_train\", auc, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"])\n",
    "        labels = self.expand_targets(batch[\"targets\"].float()) \n",
    "        labels = labels.reshape_as(outputs)\n",
    "\n",
    "\n",
    "        loss = self.criterion(outputs,labels)   \n",
    "\n",
    "        auc = self.auc(outputs, labels.int())\n",
    "\n",
    "        self.log(\"Loss_val\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"Auc_val\", auc, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        outputs = self(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"])\n",
    "        labels = self.expand_targets(batch[\"targets\"].float()) \n",
    "        labels = labels.reshape_as(outputs)\n",
    "\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        #acc = self.accuracy(outputs, labels.int())\n",
    "        auc = self.auc(outputs, labels.int())\n",
    "\n",
    "        self.log(\"Test_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"Test_auc\", auc, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_ids, dataloader_idx=None):\n",
    "\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @property\n",
    "    def chunk(self):\n",
    "        return partial(torch.split, split_size_or_sections=512, dim=1)\n",
    "\n",
    "    def expand_targets(self,targets):\n",
    "        \"\"\"This returns the Two dimentional targets given the single correct label\"\"\"\n",
    "        # 0 -> [0,1]  , 1 -> [1,0] =  (IN,OUT) class.\n",
    "        out = torch.stack([torch.tensor([1.,0.]) if val else torch.tensor([0.,1.])  for batch in  targets.bool() for val in batch])\n",
    "      \n",
    "        return out.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarobyte/envs/detests/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/jarobyte/envs/detests/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# This is required to initialize the backend-model (bert) which is a pretrained model.\n",
    "  \n",
    "pretrained_model = AutoModel.from_pretrained('../resources/checkpoints/bert-base-uncased.pt')\n",
    "tokenizer = AutoTokenizer.from_pretrained('../resources/checkpoints/bert-base-uncased-tokenizer.pt')\n",
    "model = CHUNKSUMM(model=pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loading model checkpoint\n",
    "\n",
    "first = torch.load('../resources/checkpoints/first.pt') # initial model\n",
    "second_40k = torch.load('../resources/checkpoints/second-40k.pt') # Traning with 40k sentences\n",
    "\n",
    "model.load_state_dict(second_40k) \n",
    "\n",
    "\n",
    "def get_token_scores(model,tokenizer,text:[str]):\n",
    "\n",
    "    tokenized_input = tokenizer(text,return_tensors='pt')\n",
    "    model.eval()\n",
    "\n",
    "    return tokenized_input['input_ids'],model(**tokenized_input) # (IN,OUT) probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This is a test\"\n",
    "batch_text = \"This is a test\"\n",
    "tokens,probs = get_token_scores(model,tokenizer,batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape # batch * tokens   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0006, 0.0008, 0.0007, 0.0005, 0.0007, 0.0009]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:,:,0] # batch * tokens * probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.7740e-04, 9.9942e-01],\n",
       "         [7.7118e-04, 9.9923e-01],\n",
       "         [6.7593e-04, 9.9932e-01],\n",
       "         [4.9304e-04, 9.9951e-01],\n",
       "         [6.7584e-04, 9.9932e-01],\n",
       "         [9.2531e-04, 9.9907e-01]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:,:,0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "83108396e0d77d484942493bbbef7c0a1cc37c2094a6ab1dca11eeb36016f6a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
