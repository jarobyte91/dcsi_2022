{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import pickle5 as pickle\n",
    "import importlib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsci_2022\n",
    "from dsci_2022.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/labels.pkl\", \"rb\") as file:\n",
    "#     labels = pickle.load(file)\n",
    "    \n",
    "# show(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372564, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence_score</th>\n",
       "      <th>in_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Proceedings of the 56th Annual Meeting of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Parsing has been useful for incorporating ling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Neural network-based approaches relying on den...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Generally speaking, either these approaches pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Corresponding authors: yikang.shen@umontreal.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  sentence                                               text  \\\n",
       "0         0         0  Proceedings of the 56th Annual Meeting of the ...   \n",
       "1         0         1  Parsing has been useful for incorporating ling...   \n",
       "2         0         2  Neural network-based approaches relying on den...   \n",
       "3         0         3  Generally speaking, either these approaches pr...   \n",
       "4         0         4  Corresponding authors: yikang.shen@umontreal.c...   \n",
       "\n",
       "  sentence_score  in_summary  \n",
       "0            NaN       False  \n",
       "1            NaN       False  \n",
       "2            NaN       False  \n",
       "3            NaN       False  \n",
       "4            NaN       False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_pickle(\"../data/labels.pkl\")\n",
    "show(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1171–1180 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics 1171 Devising fast and accurate constituency parsing algorithms is an important, long-standing problem in natural language processing. Parsing has been useful for incorporating linguistic prior in several related tasks, such as relation extraction, paraphrase detection (Callison-Burch, 2008), and more recently, natural language inference (Bowman et al., 2016) and machine translation (Eriguchi et al., 2017). Neural network-based approaches relying on dense input representations have recently achieved competitive results for constituency parsing (Vinyals et al., 2015; Cross and Huang, 2016; Liu and Zhang, 2017b; Stern et al., 2017a). Generally speaking, either these approaches produce the parse tree sequentially, by governing ∗Equal contribution. Corresponding authors: yikang.shen@umontreal.ca, zhouhan.lin@umontreal.ca. †Work done while at Microsoft Research, Montreal. the sequence of transitions in a transition-based parser (Nivre, 2004; Zhu et al., 2013; Chen and Manning, 2014; Cross and Huang, 2016), or use a chart-based approach by estimating non-linear potentials and performing exact structured inference by dynamic programming (Finkel et al., 2008; Durrett and Klein, 2015; Stern et al., 2017a). Transition-based models decompose the structured prediction problem into a sequence of local decisions. This enables fast greedy decoding but also leads to compounding errors because the model is never exposed to its own mistakes during training (Daumé et al., 2009). Solutions to this problem usually complexify the training procedure by using structured training through beamsearch (Weiss et al., 2015; Andor et al., 2016) and dynamic oracles (Goldberg and Nivre, 2012; Cross and Huang, 2016). On the other hand, chartbased models can incorporate structured loss functions during training and benefit from exact inference via the CYK algorithm but suffer from higher computational cost during decoding (Durrett and Klein, 2015; Stern et al., 2017a). In this paper, we propose a novel, fully-parallel model for constituency parsing, based on the concept of “syntactic distance”, recently introduced by (Shen et al., 2017) for language modeling. To construct a parse tree from a sentence, one can proceed in a top-down manner, recursively splitting larger constituents into smaller constituents, where the order of the splits defines the hierarchical structure. The syntactic distances are defined for each possible split point in the sentence. The order induced by the syntactic distances fully specifies the order in which the sentence needs to be recursively split into smaller constituents (Figure 1): in case of a binary tree, there exists a oneto-one correspondence between the ordering and the tree. Therefore, our model is trained to reproduce the ordering between split points induced by the ground-truth distances by means of a margin rank loss (Weston et al., 2011). Crucially, our model works in parallel: the estimated distance for each split point is produced independently from the others, which allows for an easy parallelization in modern parallel computing architectures for deep learning, such as GPUs. Along with the distances, we also train the model to produce the constituent labels, which are used to build the fully labeled tree. Our model is fully parallel and thus does not require computationally expensive structured inference during training. Mapping from syntactic distances to a tree can be efficiently done in O(n log n), which makes the decoding computationally attractive. Despite our strong conditional independence assumption on the output predictions, we achieve good performance for single model discriminative parsing in PTB (91.8 F1) and CTB (86.5 F1) matching, and sometimes outperforming, recent chart-based and transition-based parsing models. In this section, we start from the concept of syntactic distance introduced in Shen et al. (2017) for unsupervised parsing via language modeling and we extend it to the supervised setting. We propose two algorithms, one to convert a parse tree into a compact representation based on distances between consecutive words, and another to map the inferred representation back to a complete parse tree. The representation will later be used for supervised training. We formally define the syntactic distances of a parse tree as follows: Algorithm 1 Binary Parse Tree to Distance (∪ represents the concatenation operator of lists) 1: function DISTANCE(node) 2: if node is leaf then 3: d← [] 4: c← [] 5: t← [node.tag] 6: h← 0 7: else 8: childl, childr ← children of node 9: dl, cl, tl, hl ← Distance(childl) 10: dr, cr, tr, hr ← Distance(childr) 11: h← max(hl, hr) + 1 12: d← dl ∪ [h] ∪ dr 13: c← cl ∪ [node.label] ∪ cr 14: t← tl ∪ tr 15: end if 16: return d, c, t, h 17: end function Definition 2.1. Let T be a parse tree that contains a set of leaves (w0, ..., wn). The height of the lowest common ancestor for two leaves (wi, wj) is noted as d̃ij . The syntactic distances of T can be any vector of scalars d = (d1, ..., dn) that satisfy: sign(di − dj) = sign(d̃i−1i − d̃ j−1 j ) (1) In other words, d induces the same ranking order as the quantities d̃ji computed between pairs of consecutive words in the sequence, i.e. (d̃01, ..., d̃ n−1 n ). Note that there are n − 1 syntactic distances for a sentence of length n. Example 2.1. Consider the tree in Fig. 1 for which d̃01 = 2, d̃ 1 2 = 1. An example of valid syntactic distances for this tree is any d = (d1, d2) such that d1 > d2. Given this definition, the parsing model predicts a sequence of scalars, which is a more natural setting for models based on neural networks, rather than predicting a set of spans. For comparison, in most of the current neural parsing methods, the model needs to output a sequence of transitions (Cross and Huang, 2016; Chen and Manning, 2014). Let us first consider the case of a binary parse tree. Algorithm 1 provides a way to convert it to a tuple (d, c, t), where d contains the height of the inner nodes in the tree following a left-to-right (in order) traversal, c the constituent labels for each node in the same order and t the part-of-speech Algorithm 2 Distance to Binary Parse Tree 1: function TREE(d,c,t) 2: if d = [] then 3: node← Leaf(t) 4: else 5: i← argmaxi(d) 6: childl ← Tree(d<i, c<i, t<i) 7: childr ← Tree(d>i, c>i, t≥i) 8: node← Node(childl, childr, ci) 9: end if 10: return node 11: end function (POS) tags of each word in the left-to-right order. d is a valid vector of syntactic distances satisfying Definition 2.1. Once a model has learned to predict these variables, Algorithm 2 can reconstruct a unique binary tree from the output of the model (d̂, ĉ, t̂). The idea in Algorithm 2 is similar to the top-down parsing method proposed by Stern et al. (2017a), but differs in one key aspect: at each recursive call, there is no need to estimate the confidence for every split point. The algorithm simply chooses the split point i with the maximum d̂i, and assigns to the span the predicted label ĉi. This makes the running time of our algorithm to be inO(n log n), compared to theO(n2) of the greedy top-down algorithm by (Stern et al., 2017a). Figure 2 shows an example of the reconstruction of parse tree. Alternatively, the tree reconstruction process can also be done in a bottom-up manner, which requires the recursive composition of adjacent spans according to the ranking induced by their syntactic distance, a process akin to agglomerative clustering. One potential issue is the existence of unary and n-ary nodes. We follow the method proposed by Stern et al. (2017a) and add a special empty label ∅ to spans that are not themselves full constituents but simply arise during the course of implicit binarization. For the unary nodes that contains one nonterminal node, we take the common approach of treating these as additional atomic labels alongside all elementary nonterminals (Stern et al., 2017a). For all terminal nodes, we determine whether it belongs to a unary chain or not by predicting an additional label. If it is predicted with a label different from the empty label, we conclude that it is a direct child of a unary constituent with that label. Otherwise if it is predicted to have an empty label, we conclude that it is a child of a bigger constituent which has other constituents or words as its siblings. An n-ary node can arbitrarily be split into binary nodes. We choose to use the leftmost split point. The split point may also be chosen based on model prediction during training. Recovering an n-ary parse tree from the predicted binary tree simply requires removing the empty nodes and split combined labels corresponding to unary chains. Algorithm 2 is a divide-and-conquer algorithm. The running time of this procedure is O(n log n). However, the algorithm is naturally adapted for execution in a parallel environment, which can further reduce its running time to O(log n). We use neural networks to estimate the vector of syntactic distances for a given sentence. We use a modified hinge loss, where the target distances are generated by the tree-to-distance conversion given by Algorithm 1. Section 3.1 will describe in detail the model architecture, and Section 3.2 describes the loss we use in this setting. Given input words w = (w0, w1, ..., wn), we predict the tuple (d, c, t). The POS tags t are given by an external Part-Of-Speech (POS) tagger. The syntactic distances d and constituent labels c are predicted using a neural network architecture that stacks recurrent (LSTM (Hochreiter and Schmidhuber, 1997)) and convolutional layers. Words and tags are first mapped to sequences of embeddings ew0 , ..., e w n and e t 0, ..., e t n. Then the word embeddings and the tag embeddings are concatenated together as inputs for a stack of bidirectional LSTM layers: hw0 , ...,h w n = BiLSTMw([e w 0 , e t 0], ..., [e w n , e t n]) (2) where BiLSTMw(·) is the word-level bidirectional layer, which gives the model enough capacity to capture long-term syntactical relations between words. To predict the constituent labels for each word, we pass the hidden states representations hw0 , ...,h w n through a 2-layer network FF w c , with softmax output: p(cwi |w) = softmax(FFwc (hwi )) (3) To compose the necessary information for inferring the syntactic distances and the constituency label information, we perform an additional convolution: gs1, . . . ,g s n = CONV(h w 0 , ...,h w n ) (4) where gsi can be seen as a draft representation for each split position in Algorithm 2. Note that the subscripts of gsi s start with 1, since we have n− 1 positions as non-terminal constituents. Then, we stack a bidirectional LSTM layer on top of gsi : hs1, ...,h s n = BiLSTMs(g s 1, . . . ,g s n) (5) where BiLSTMs fine-tunes the representation by conditioning on other split position representations. Interleaving between LSTM and convolution layers turned out empirically to be the best choice over multiple variations of the model, including using self-attention (Vaswani et al., 2017) instead of LSTM. To calculate the syntactic distances for each position, the vectors hs1, . . . ,h s n are transformed through a 2-layer feed-forward network FFd with a single output unit (this can be done in parallel with 1x1 convolutions), with no activation function at the output layer: d̂i = FFd(hsi ), (6) For predicting the constituent labels, we pass the same representations hs1, . . . ,h s n through another 2-layer network FFsc, with softmax output. p(csi |w) = softmax(FFsc(hsi)) (7) The overall architecture is shown in Figure 2a. Since the output (d, c, t) can be unambiguously transfered to a unique parse tree, the model implicitly makes all parsing decisions inside the recurrent and convolutional layers. Given a set of training examples D = {〈dk, ck, tk,wk〉}Kk=1, the training objective is the sum of the prediction losses of syntactic distances dk and constituent labels ck. Due to the categorical nature of variable c, we use a standard softmax classifier with a crossentropy loss Llabel for constituent labels, using the estimated probabilities obtained in Eq. 3 and 7. A naïve loss function for estimating syntactic distances is the mean-squared error (MSE): Lmsedist = ∑ i (di − d̂i)2 (8) The MSE loss forces the model to regress on the exact value of the true distances. Given that only the ranking induced by the ground-truth distances in d is important, as opposed to the absolute values themselves, using an MSE loss over-penalizes the model by ignoring ranking equivalence between different predictions. Therefore, we propose to minimize a pair-wise learning-to-rank loss, similar to those proposed in (Burges et al., 2005). We define our loss as a variant of the hinge loss as: Lrankdist = ∑ i,j>i [1− sign(di − dj)(d̂i − d̂j)]+, (9) where [x]+ is defined as max(0, x). This loss encourages the model to reproduce the full ranking order induced by the ground-truth distances. The final loss for the overall model is just the sum of individual losses L = Llabel + Lrankdist . We evaluate our model described above on 2 different datasets, the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) dataset, and the Chinese Treebank (CTB) dataset. For evaluating the F1 score, we use the standard evalb1 tool. We provide both labeled and unlabeled F1 score, where the former takes into consideration the constituent label for each predicted 1http://nlp.cs.nyu.edu/evalb/ constituent, while the latter only considers the position of the constituents. In the tables below, we report the labeled F1 scores for comparison with previous work, as this is the standard metric usually reported in the relevant literature. For the PTB experiments, we follow the standard train/valid/test separation and use sections 2-21 for training, section 22 for development and section 23 for test set. Following this split, the dataset has 45K training sentences and 1700, 2416 sentences for valid/test respectively. The placeholders with the -NONE- tag are stripped from the dataset during preprocessing. The POS tags are predicted with the Stanford Tagger (Toutanova et al., 2003). We use a hidden size of 1200 for each direction on all LSTMs, with 0.3 dropout in all the feedforward connections, and 0.2 recurrent connection dropout (Merity et al., 2017). The convolutional filter size is 2. The number of convolutional channels is 1200. As a common practice for neural network based NLP models, the embedding layer that maps word indexes to word embeddings is randomly initialized. The word embeddings are sized 400. Following (Merity et al., 2017), we randomly swap an input word embedding during training with the zero vector with probability of 0.1. We found this helped the model to generalize better. Training is conducted with Adam algorithm with l2 regularization decay 1 × 10−6. We pick the result obtaining the highest labeled F1 on the validation set, and report the corresponding test F1, together with other statistics. We report our results in Table 1. Our best model obtains a labeled F1 score of 91.8 on the test set (Table 1). Detailed dev/test set performances, including label accuracy is reported in Table 3. Our model performs achieves good performance for single-model constituency parsing trained without external data. The best result from (Stern et al., 2017b) is obtained by a generative model. Very recently, we came to knowledge of Gaddy et al. (2018), which uses character-level LSTM features coupled with chart-based parsing to improve performance. Similar sub-word features can be also used in our model. We leave this investigation for future works. For comparison, other models obtaining better scores either use ensembles, benefit from semi-supervised learning, or recur to re-ranking of a set of candidates. We use the Chinese Treebank 5.1 dataset, with articles 001-270 and 440-1151 for training, articles 301-325 as development set, and articles 271-300 for test set. This is a standard split in the literature (Liu and Zhang, 2017b). The -NONE- tags are stripped as well. The hidden size for the LSTM networks is set to 1200. We use a dropout rate of 0.4 on the feed-forward connections, and 0.1 recurrent connection dropout. The convolutional layer has 1200 channels, with a filter size of 2. We use 400 dimensional word embeddings. During training, input word embeddings are randomly swapped with the zero vector with probability of 0.1. We also apply a l2 regularization weighted by 1×10−6 on the parameters of the network. Table 2 reports our results compared to other benchmarks. To the best of our knowledge, we set a new stateof-the-art for single-model parsing achieving 86.5 F1 on the test set. The detailed statistics are shown in Table 3. We perform an ablation study by removing components from a network trained with the best set of hyperparameters, and re-train the ablated version from scratch. This gives an idea of the relative contributions of each of the components in the model. Results are reported in Table 4. It seems that the top LSTM layer has a relatively big impact on performance. This may give additional capacity to the model for capturing long-term dependencies useful for label prediction. We also exper- imented by using 300D GloVe (Pennington et al., 2014) embedding for the input layer but this didn’t yield improvements over the model’s best performance. Unsurprisingly, the model trained with MSE loss underperforms considerably a model trained with the rank loss. The prediction of syntactic distances can be batched in modern GPU architectures. The distance to tree conversion is a O(n log n) (n stand for the number of words in the input sentence) divide-and-conquer algorithm. We compare the parsing speed of our parser with other state-ofthe-art neural parsers in Table 5. As the syntactic distance computation can be performed in parallel within a GPU, we first compute the distances in a batch, then we iteratively decode the tree with Algorithm 2. It is worth to note that this comparison may be unfair since some of the reported results may use very different hardware settings. We couldn’t find the source code to re-run them on our hardware, to give a fair enough comparison. In our setting, we use an NVIDIA TITAN Xp graphics card for running the neural network part, and the distance to tree inference is run on an Intel Core i7-6850K CPU, with 3.60GHz clock speed. Parsing natural language with neural network models has recently received growing attention. These models have attained state-of-the-art results for dependency parsing (Chen and Manning, 2014) and constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Coavoux and Crabbé, 2016). Early work in neural network based parsing directly use a feed-forward neural network to predict parse trees (Chen and Manning, 2014). Vinyals et al. (2015) use a sequence-tosequence framework where the decoder outputs a linearized version of the parse tree given an input sentence. Generally, in these models, the correctness of the output tree is not strictly ensured (although empirically observed). Other parsing methods ensure structural consistency by operating in a transition-based setting (Chen and Manning, 2014) by parsing either in the top-down direction (Dyer et al., 2016; Liu and Zhang, 2017b), bottom-up (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016) and recently in-order (Liu and Zhang, 2017a). Transition-based methods generally suffer from compounding errors due to exposure bias: during testing, the model is exposed to a very different regime (i.e. decisions sampled from the model itself) than what was encountered during training (i.e. the ground-truth decisions) (Daumé et al., 2009; Goldberg and Nivre, 2012). This can have catastrophic effects on test performance but can be mitigated to a certain extent by using beamsearch instead of greedy decoding. (Stern et al., 2017b) proposes an effective inference method for generative parsing, which enables direct decoding in those models. More complex training methods have been devised in order to alleviate this problem (Goldberg and Nivre, 2012; Cross and Huang, 2016). Other efforts have been put into neural chart-based parsing (Durrett and Klein, 2015; Stern et al., 2017a) which ensure structural consistency and offer exact inference with CYK algorithm. (Gaddy et al., 2018) includes a simplified CYK-style inference, but the complexity still remains in O(n3). In this work, our model learns to produce a particular representation of a tree in parallel. Representations can be computed in parallel, and the conversion from representation to a full tree can efficiently be done with a divide-and-conquer algorithm. As our model outputs decisions in parallel, our model doesn’t suffer from the exposure bias. Interestingly, a series of recent works, both in machine translation (Gu et al., 2018) and speech synthesis (Oord et al., 2017), considered the sequence of output variables conditionally independent given the inputs. We presented a novel constituency parsing scheme based on predicting real-valued scalars, named syntactic distances, whose ordering identify the sequence of top-down split decisions. We employ a neural network model that predicts the distances d and the constituent labels c. Given the algorithms presented in Section 2, we can build an unambiguous mapping between each (d, c, t) and a parse tree. One peculiar aspect of our model is that it predicts split decisions in parallel. Our experiments show that our model can achieve strong performance compare to previous models, while being significantly more efficient. Since the architecture of model is no more than a stack of standard recurrent and convolution layers, which are essential components in most academic and industrial deep learning frameworks, the deployment of this method would be straightforward. The authors would like to thank Compute Canada for providing the computational resources. The authors would also like to thank Jackie Chi Kit Cheung for the helpful discussions. Zhouhan Lin would like to thank AdeptMind for generously supporting his research via scholarship. \n"
     ]
    }
   ],
   "source": [
    "example_raw = labels.query(\"paper_id == 0\")\\\n",
    ".sort_values(\"sentence\")\\\n",
    ".text.map(lambda x: re.sub(r\"\\s\", \" \", x) + \" \").sum()\n",
    "print(example_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Proceedings', 0.5),\n",
       " ('of', 0.5086421904134987),\n",
       " ('the', 0.5172817987813012),\n",
       " ('56th', 0.5259162438291546),\n",
       " ('Annual', 0.534542945825462),\n",
       " ('Meeting', 0.5431593273520349),\n",
       " ('of', 0.5517628140741532),\n",
       " ('the', 0.5603508355097054),\n",
       " ('Association', 0.5689208257971765),\n",
       " ('for', 0.5774702244622562)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = [\n",
    "    (w, 0.5 + math.sin(10 * 2 * math.pi * i / len(example_raw.split(\" \"))) / 2) \n",
    "    for i, w in enumerate(example_raw.split(\" \"))\n",
    "]\n",
    "print(len(example))\n",
    "plt.plot([s for w, s in example])\n",
    "example[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108, 109, 108, 109, 108, 109, 108, 109, 108, 109]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('important,', 0.805308384784835),\n",
       " ('long-standing', 0.8121067532766586),\n",
       " ('problem', 0.8188118729669054),\n",
       " ('in', 0.8254217405526382),\n",
       " ('natural', 0.8319343811895946),\n",
       " ('language', 0.8383478490822156),\n",
       " ('processing.', 0.8446602280649946),\n",
       " ('Parsing', 0.8508696321749751),\n",
       " ('has', 0.8569742062152228),\n",
       " ('been', 0.8629721263091079)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dsci_2022.utils)\n",
    "spans = dsci_2022.utils.extract_spans(example, 0.8)\n",
    "print([len(l) for l in spans])\n",
    "spans[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "important, long-standing problem in natural language processing. Parsing has been useful for incorporating linguistic prior in several related tasks, such as relation extraction, paraphrase detection (Callison-Burch, 2008), and more recently, natural language inference (Bowman et al., 2016) and machine translation (Eriguchi et al., 2017). Neural network-based approaches relying on dense input representations have recently achieved competitive results for constituency parsing (Vinyals et al., 2015; Cross and Huang, 2016; Liu and Zhang, 2017b; Stern et al., 2017a). Generally speaking, either these approaches produce the parse tree sequentially, by governing ∗Equal contribution. Corresponding authors: yikang.shen@umontreal.ca, zhouhan.lin@umontreal.ca. †Work done while at Microsoft Research, Montreal. the sequence of transitions in a transition-based\n",
      "\n",
      "in case of a binary tree, there exists a oneto-one correspondence between the ordering and the tree. Therefore, our model is trained to reproduce the ordering between split points induced by the ground-truth distances by means of a margin rank loss (Weston et al., 2011). Crucially, our model works in parallel: the estimated distance for each split point is produced independently from the others, which allows for an easy parallelization in modern parallel computing architectures for deep learning, such as GPUs. Along with the distances, we also train the model to produce the constituent labels, which are used to build the fully labeled tree. Our model is fully parallel\n",
      "\n",
      "parse tree that contains a set of leaves (w0, ..., wn). The height of the lowest common ancestor for two leaves (wi, wj) is noted as d̃ij . The syntactic distances of T can be any vector of scalars d = (d1, ..., dn) that satisfy: sign(di − dj) = sign(d̃i−1i − d̃ j−1 j ) (1) In other words, d induces the same ranking order as the quantities d̃ji computed between pairs of consecutive words in the sequence, i.e. (d̃01, ..., d̃ n−1 n ). Note that there are n − 1 syntactic distances for a sentence of length n. Example 2.1. Consider the tree in Fig.\n",
      "\n",
      "by Stern et al. (2017a), but differs in one key aspect: at each recursive call, there is no need to estimate the confidence for every split point. The algorithm simply chooses the split point i with the maximum d̂i, and assigns to the span the predicted label ĉi. This makes the running time of our algorithm to be inO(n log n), compared to theO(n2) of the greedy top-down algorithm by (Stern et al., 2017a). Figure 2 shows an example of the reconstruction of parse tree. Alternatively, the tree reconstruction process can also be done in a bottom-up manner, which requires the recursive composition of adjacent spans according to the\n",
      "\n",
      "n). We use neural networks to estimate the vector of syntactic distances for a given sentence. We use a modified hinge loss, where the target distances are generated by the tree-to-distance conversion given by Algorithm 1. Section 3.1 will describe in detail the model architecture, and Section 3.2 describes the loss we use in this setting. Given input words w = (w0, w1, ..., wn), we predict the tuple (d, c, t). The POS tags t are given by an external Part-Of-Speech (POS) tagger. The syntactic distances d and constituent labels c are predicted using a neural network architecture that stacks recurrent (LSTM (Hochreiter and Schmidhuber, 1997)) and\n",
      "\n",
      "model, including using self-attention (Vaswani et al., 2017) instead of LSTM. To calculate the syntactic distances for each position, the vectors hs1, . . . ,h s n are transformed through a 2-layer feed-forward network FFd with a single output unit (this can be done in parallel with 1x1 convolutions), with no activation function at the output layer: d̂i = FFd(hsi ), (6) For predicting the constituent labels, we pass the same representations hs1, . . . ,h s n through another 2-layer network FFsc, with softmax output. p(csi |w) = softmax(FFsc(hsi)) (7) The overall architecture is shown in Figure 2a. Since the output (d, c, t) can be\n",
      "\n",
      "the Penn Treebank (PTB) dataset, and the Chinese Treebank (CTB) dataset. For evaluating the F1 score, we use the standard evalb1 tool. We provide both labeled and unlabeled F1 score, where the former takes into consideration the constituent label for each predicted 1http://nlp.cs.nyu.edu/evalb/ constituent, while the latter only considers the position of the constituents. In the tables below, we report the labeled F1 scores for comparison with previous work, as this is the standard metric usually reported in the relevant literature. For the PTB experiments, we follow the standard train/valid/test separation and use sections 2-21 for training, section 22 for development and section 23 for test set. Following\n",
      "\n",
      "uses character-level LSTM features coupled with chart-based parsing to improve performance. Similar sub-word features can be also used in our model. We leave this investigation for future works. For comparison, other models obtaining better scores either use ensembles, benefit from semi-supervised learning, or recur to re-ranking of a set of candidates. We use the Chinese Treebank 5.1 dataset, with articles 001-270 and 440-1151 for training, articles 301-325 as development set, and articles 271-300 for test set. This is a standard split in the literature (Liu and Zhang, 2017b). The -NONE- tags are stripped as well. The hidden size for the LSTM networks is set to 1200. We use a\n",
      "\n",
      "We compare the parsing speed of our parser with other state-ofthe-art neural parsers in Table 5. As the syntactic distance computation can be performed in parallel within a GPU, we first compute the distances in a batch, then we iteratively decode the tree with Algorithm 2. It is worth to note that this comparison may be unfair since some of the reported results may use very different hardware settings. We couldn’t find the source code to re-run them on our hardware, to give a fair enough comparison. In our setting, we use an NVIDIA TITAN Xp graphics card for running the neural network part, and the distance to\n",
      "\n",
      "More complex training methods have been devised in order to alleviate this problem (Goldberg and Nivre, 2012; Cross and Huang, 2016). Other efforts have been put into neural chart-based parsing (Durrett and Klein, 2015; Stern et al., 2017a) which ensure structural consistency and offer exact inference with CYK algorithm. (Gaddy et al., 2018) includes a simplified CYK-style inference, but the complexity still remains in O(n3). In this work, our model learns to produce a particular representation of a tree in parallel. Representations can be computed in parallel, and the conversion from representation to a full tree can efficiently be done with a divide-and-conquer algorithm. As our model outputs decisions\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(dsci_2022.utils)\n",
    "summary = dsci_2022.utils.produce_summary(\n",
    "    spans, \n",
    "    summary_separator = \"\\n\\n\"\n",
    "#     summary_separator = \"\\n---\\n\"\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6759, 5898], [6044, 5183])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_1 = summary\n",
    "hyp_1 = \"\\n\\n\".join(summary.split(\"\\n\\n\")[:-1])\n",
    "ref_2 = \"\\n\\n\".join(summary.split(\"\\n\\n\")[1:])\n",
    "hyp_2 = \"\\n\\n\".join(summary.split(\"\\n\\n\")[1:-1])\n",
    "\n",
    "refs = [ref_1, ref_2]\n",
    "hyps = [hyp_1, hyp_2]\n",
    "\n",
    "[len(s) for s in refs], [len(s) for s in hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.9271402550091075, 'p': 1.0, 'f': 0.9621928116423076},\n",
       "  'rouge-2': {'r': 0.9071428571428571, 'p': 1.0, 'f': 0.9513108564350743},\n",
       "  'rouge-l': {'r': 0.9271402550091075, 'p': 1.0, 'f': 0.9621928116423076}},\n",
       " {'rouge-1': {'r': 0.9016393442622951, 'p': 1.0, 'f': 0.9482758570823425},\n",
       "  'rouge-2': {'r': 0.8946772366930917, 'p': 1.0, 'f': 0.9444112323137172},\n",
       "  'rouge-l': {'r': 0.9016393442622951, 'p': 1.0, 'f': 0.9482758570823425}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps = hyps, refs = refs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>metric</th>\n",
       "      <th>f-measure</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rouge-1</td>\n",
       "      <td>0.962193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.927140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>rouge-2</td>\n",
       "      <td>0.951311</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>rouge-l</td>\n",
       "      <td>0.962193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.927140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>rouge-1</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.901639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rouge-2</td>\n",
       "      <td>0.944411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>rouge-l</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.901639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example   metric  f-measure  precision    recall\n",
       "0        0  rouge-1   0.962193        1.0  0.927140\n",
       "1        0  rouge-2   0.951311        1.0  0.907143\n",
       "2        0  rouge-l   0.962193        1.0  0.927140\n",
       "3        1  rouge-1   0.948276        1.0  0.901639\n",
       "4        1  rouge-2   0.944411        1.0  0.894677\n",
       "5        1  rouge-l   0.948276        1.0  0.901639"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dsci_2022)\n",
    "importlib.reload(dsci_2022.metrics)\n",
    "\n",
    "results = dsci_2022.metrics.compute_rouge(refs, hyps)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>f-measure</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge-1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.955234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.91439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.947861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-l</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.955234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.91439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         example  f-measure  precision   recall\n",
       "metric                                         \n",
       "rouge-1      0.5   0.955234        1.0  0.91439\n",
       "rouge-2      0.5   0.947861        1.0  0.90091\n",
       "rouge-l      0.5   0.955234        1.0  0.91439"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby(\"metric\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
